{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "apiKey=\"4CiQzYxwK7Q70iCQm2I8\"\n",
        "!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "import os\n",
        "\n",
        "rf = Roboflow(api_key=apiKey)\n",
        "project = rf.workspace(\"gemastik-rldu5\").project(\"human-thermal-detection-regtz\")\n",
        "version = project.version(5)\n",
        "dataset = version.download(\"yolov12\")\n",
        "locDataset = os.path.join(dataset.location, 'data.yaml')"
      ],
      "metadata": {
        "id": "YnD15S7OVHfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838efae2-a599-474c-b21b-2635a0bbc7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.7.14)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif<2 (from roboflow)\n",
            "  Downloading pillow_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
            "Downloading roboflow-1.2.1-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-avif-plugin-1.5.2 pillow-heif-1.0.0 python-dotenv-1.1.1 roboflow-1.2.1\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Human-Thermal-Detection-5 to yolov12:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242298/242298 [00:04<00:00, 58436.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Human-Thermal-Detection-5 in yolov12:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9760/9760 [00:01<00:00, 7179.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-njLw2mZnxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299a5493-3b65-40b5-c0c1-4165cfc21ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IILVEaqvc8sD"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/runs/detect'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EdB0wTLHOF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be6191a-6f07-4d83-d186-570562ec25b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.167-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.167-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.167 ultralytics-thop-2.0.14\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!pip install ultralytics\n",
        "from ultralytics import YOLO, RTDETR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_2oN5qPE7u3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7022d8-7cdd-41cd-e858-20c87c713077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting new YOLOv12-L training...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12l.pt to 'yolo12l.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51.2M/51.2M [00:00<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.167 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Human-Thermal-Detection-5/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/runs/detect/train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
            "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
            "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  6                  -1  4   4272944  ultralytics.nn.modules.block.A2C2f           [512, 512, 4, True, 4, True, 1.2]\n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  4   4272944  ultralytics.nn.modules.block.A2C2f           [512, 512, 4, True, 1, True, 1.2]\n",
            "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 11                  -1  2   2102784  ultralytics.nn.modules.block.A2C2f           [1024, 512, 2, False, -1, True, 1.2]\n",
            " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 14                  -1  2    592640  ultralytics.nn.modules.block.A2C2f           [1024, 256, 2, False, -1, True, 1.2]\n",
            " 15                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  2   2037248  ultralytics.nn.modules.block.A2C2f           [768, 512, 2, False, -1, True, 1.2]\n",
            " 18                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
            " 21        [14, 17, 20]  1   1411795  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n",
            "YOLOv12l summary: 488 layers, 26,389,875 parameters, 26,389,859 gradients, 89.4 GFLOPs\n",
            "\n",
            "Transferred 1239/1245 items from pretrained weights\n",
            "Freezing layer 'model.21.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1811.2Â±854.2 MB/s, size: 62.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Human-Thermal-Detection-5/train/labels.cache... 4016 images, 1689 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4016/4016 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 443.9Â±215.8 MB/s, size: 32.8 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Human-Thermal-Detection-5/valid/labels.cache... 570 images, 218 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/drive/MyDrive/runs/detect/train4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 205 weight(decay=0.0), 214 weight(decay=0.0005), 211 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/runs/detect/train4\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/50      9.44G      2.248      2.021      1.137         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:32<00:00,  1.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.579      0.542      0.545      0.225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/50      9.64G      2.268      1.802      1.136         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.494      0.423      0.397      0.145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/50      9.68G       2.23      1.782      1.121         78        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:17<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467       0.63      0.568      0.595      0.229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/50      9.68G      2.152      1.661      1.094         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:18<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.497      0.512      0.473      0.195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/50      9.68G      2.114      1.569      1.087         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:16<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.591        0.6      0.568      0.227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/50      9.69G      2.052      1.504       1.07         66        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:17<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.688      0.669      0.712      0.309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/50      9.64G      2.041      1.466      1.065         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:18<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.702      0.695      0.737      0.329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/50      9.69G      2.024      1.462      1.053         67        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.716      0.662      0.723      0.316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/50      9.67G      2.022       1.46      1.047         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.704      0.672      0.726      0.332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/50      9.69G      1.978      1.396      1.043         65        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.782      0.726      0.781      0.361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/50      9.65G      1.969      1.364      1.039         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:17<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.737      0.639      0.707      0.325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/50      9.67G      1.941      1.317      1.023         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:16<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.737      0.684      0.741      0.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/50      9.65G      1.931      1.306      1.024         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:17<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.698      0.694      0.738      0.309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/50      9.68G      1.935      1.336       1.03         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:17<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.847      0.707      0.798       0.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/50      9.63G      1.919      1.284      1.017         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.804      0.756      0.827      0.396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/50      9.68G        1.9      1.277      1.014         64        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.822       0.73      0.807      0.399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/50      9.68G      1.893       1.26      1.013         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.815      0.681      0.794      0.396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/50      9.68G      1.894      1.252       1.01         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:18<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.818      0.762      0.841      0.417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/50      9.62G      1.868       1.22      1.007         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.789      0.732      0.814      0.414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/50      9.68G      1.877      1.231      1.003         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.816      0.755      0.833      0.416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/50      9.69G      1.852      1.187     0.9963         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.842      0.782      0.851      0.432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/50      9.66G      1.828      1.161     0.9929         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.814      0.769      0.846      0.428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/50      9.65G       1.83      1.185     0.9894         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.812      0.795      0.855      0.432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/50      9.68G      1.823      1.153     0.9978         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:22<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.797      0.804      0.849      0.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/50      9.67G      1.839      1.171      1.003         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.846      0.792      0.864      0.446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/50      9.68G      1.828      1.135     0.9915         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.832      0.759       0.85      0.436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/50      9.67G      1.812      1.144     0.9891         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.847      0.802      0.863       0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/50      9.67G      1.813      1.117     0.9897         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.843      0.802      0.867      0.444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/50      9.69G      1.782      1.165      0.986         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.855      0.814      0.881      0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/50      9.69G      1.787        1.1      0.983         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:22<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.851       0.81      0.878      0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/50      9.64G      1.795       1.09     0.9892         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.861       0.81      0.888      0.459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/50      9.68G      1.776      1.098     0.9803         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.851      0.817      0.876      0.452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/50      9.68G      1.779      1.084      0.977         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.857      0.817      0.873      0.444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/50      9.67G      1.781      1.087     0.9795         59        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.858      0.803      0.878      0.465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/50      9.62G      1.755      1.045      0.978         76        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:22<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.855      0.825      0.887      0.463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      36/50      9.72G      1.768      1.037     0.9751         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:22<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.851      0.809      0.875      0.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      37/50      9.66G      1.765      1.043     0.9779         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.858      0.817      0.883      0.462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      38/50      9.67G      1.745      1.024     0.9766         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.878      0.819      0.887      0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      39/50      9.68G      1.744      1.024     0.9681         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.874      0.836      0.895      0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      40/50      9.67G      1.752       1.03     0.9733         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:22<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.862       0.82      0.887       0.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      41/50      9.68G      1.685      1.033     0.9856          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:17<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.859      0.843      0.894      0.476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      42/50      9.69G      1.693      1.017     0.9912         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:21<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.864      0.833       0.89      0.473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      43/50      9.64G      1.679     0.9799     0.9915         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:18<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.861       0.86        0.9      0.485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      44/50      9.67G      1.671      0.985      0.983         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:18<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.866      0.834       0.89      0.478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      45/50      9.68G      1.665     0.9564     0.9832         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:18<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467       0.87      0.851      0.904      0.491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      46/50      9.68G      1.652     0.9686     0.9762         76        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.875      0.852      0.903      0.479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      47/50      9.63G      1.656     0.9448     0.9811         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.871      0.857      0.909      0.488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      48/50      9.69G      1.643     0.9342     0.9758         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.879      0.859      0.905       0.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/50      9.67G      1.642     0.9175     0.9755         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:20<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.875      0.851       0.91      0.492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/50      9.69G      1.644     0.9261     0.9775         54        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:16<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.877      0.858       0.91      0.492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "50 epochs completed in 4.777 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/runs/detect/train4/weights/last.pt, 53.5MB\n",
            "Optimizer stripped from /content/drive/MyDrive/runs/detect/train4/weights/best.pt, 53.5MB\n",
            "\n",
            "Validating /content/drive/MyDrive/runs/detect/train4/weights/best.pt...\n",
            "Ultralytics 8.3.167 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLOv12l summary (fused): 283 layers, 26,339,843 parameters, 0 gradients, 88.5 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:17<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.876      0.861      0.911      0.492\n",
            "Speed: 0.4ms preprocess, 24.3ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/runs/detect/train4\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7bf721d8e110>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,\n",
              "            0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,\n",
              "            0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,\n",
              "            0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,\n",
              "            0.99573,     0.99573,     0.99573,     0.99219,     0.99219,     0.99219,     0.99219,     0.99219,     0.99219,     0.99219,     0.99219,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,\n",
              "             0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,      0.9914,\n",
              "             0.9914,      0.9914,      0.9914,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,\n",
              "            0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,     0.99123,\n",
              "            0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,     0.99038,\n",
              "            0.99038,     0.99038,     0.98866,     0.98866,     0.98866,     0.98718,     0.98718,     0.98718,     0.98718,     0.98718,     0.98718,     0.98718,     0.98551,     0.98551,     0.98424,     0.98424,     0.98424,     0.98424,     0.98424,     0.98424,     0.98424,     0.98403,     0.98403,\n",
              "            0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98403,     0.98307,     0.98307,     0.98307,\n",
              "            0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,\n",
              "            0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98307,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,\n",
              "            0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,     0.98286,\n",
              "            0.98286,     0.98286,     0.98286,     0.98286,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,\n",
              "            0.98171,     0.98171,     0.98171,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98126,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,\n",
              "            0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,\n",
              "            0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,\n",
              "            0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98048,     0.98036,     0.98036,     0.98036,     0.98036,     0.98036,     0.98036,     0.98036,     0.98036,     0.98036,\n",
              "            0.98036,     0.98036,     0.98036,     0.98036,     0.98036,     0.98036,     0.98036,     0.98036,     0.97959,     0.97959,     0.97959,     0.97926,     0.97926,     0.97926,     0.97926,     0.97926,     0.97926,     0.97926,     0.97926,     0.97926,     0.97926,     0.97926,     0.97845,\n",
              "            0.97804,     0.97804,     0.97804,     0.97804,     0.97804,     0.97804,     0.97804,     0.97804,     0.97804,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,     0.97791,\n",
              "            0.97791,     0.97729,     0.97729,     0.97729,     0.97729,     0.97669,     0.97669,     0.97669,     0.97669,     0.97669,     0.97617,     0.97617,     0.97617,     0.97617,     0.97617,     0.97544,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,\n",
              "            0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97523,     0.97485,     0.97485,\n",
              "            0.97485,     0.97485,     0.97485,     0.97485,     0.97485,     0.97455,     0.97455,     0.97455,     0.97455,     0.97455,     0.97455,     0.97455,     0.97455,     0.97455,     0.97386,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,\n",
              "             0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,\n",
              "             0.9722,      0.9722,      0.9722,      0.9722,      0.9722,      0.9722,     0.97156,     0.97097,     0.97035,     0.96911,     0.96867,     0.96867,     0.96867,     0.96867,     0.96834,     0.96834,     0.96834,     0.96834,     0.96834,     0.96834,     0.96795,     0.96795,     0.96795,\n",
              "            0.96795,     0.96795,     0.96684,     0.96684,     0.96684,      0.9663,     0.96622,     0.96622,     0.96622,     0.96622,     0.96622,     0.96622,     0.96622,     0.96622,     0.96622,     0.96622,     0.96582,     0.96582,     0.96582,     0.96582,      0.9653,      0.9653,     0.96504,\n",
              "            0.96504,     0.96504,     0.96504,     0.96504,     0.96504,     0.96504,     0.96451,     0.96395,     0.96342,     0.96287,     0.96253,     0.96253,     0.96253,     0.96253,      0.9622,      0.9622,      0.9622,      0.9622,      0.9622,     0.96124,     0.96124,     0.96124,     0.96012,\n",
              "            0.95969,     0.95969,     0.95921,     0.95921,     0.95912,     0.95912,     0.95912,     0.95912,     0.95912,     0.95912,     0.95912,     0.95912,     0.95804,     0.95757,     0.95757,     0.95731,     0.95731,     0.95731,     0.95731,     0.95624,     0.95589,     0.95589,     0.95589,\n",
              "            0.95544,     0.95544,     0.95507,     0.95507,     0.95507,     0.95405,     0.95353,     0.95273,     0.95273,     0.95273,     0.95273,     0.95238,     0.95238,     0.95238,      0.9509,      0.9509,      0.9507,      0.9507,      0.9507,      0.9507,      0.9507,     0.95033,     0.95033,\n",
              "            0.94994,     0.94994,     0.94944,     0.94914,     0.94914,     0.94914,      0.9489,      0.9489,      0.9489,      0.9489,     0.94863,     0.94863,     0.94863,     0.94834,     0.94834,     0.94834,     0.94834,      0.9474,     0.94694,     0.94657,     0.94657,     0.94618,     0.94579,\n",
              "            0.94579,     0.94545,     0.94545,     0.94507,     0.94507,     0.94456,     0.94456,     0.94456,     0.94456,     0.94456,     0.94456,     0.94456,     0.94433,     0.94433,     0.94433,     0.94433,      0.9441,      0.9441,      0.9441,     0.94363,     0.94197,     0.94197,     0.94197,\n",
              "            0.94197,     0.94066,     0.94066,     0.93882,     0.93792,      0.9375,     0.93667,     0.93575,     0.93563,     0.93563,     0.93563,     0.93563,     0.93542,     0.93542,     0.93542,     0.93514,     0.93514,     0.93514,      0.9343,      0.9335,      0.9335,     0.93323,     0.93323,\n",
              "            0.93244,     0.93244,     0.93162,     0.93034,     0.92959,     0.92959,      0.9292,     0.92885,     0.92805,     0.92728,     0.92728,      0.9269,     0.92677,     0.92677,     0.92677,     0.92636,     0.92612,     0.92612,     0.92582,     0.92582,     0.92552,     0.92422,     0.92311,\n",
              "            0.92311,     0.92227,     0.92064,     0.92064,     0.92064,     0.92064,     0.92064,     0.92064,     0.92036,     0.91996,     0.91886,     0.91886,     0.91811,     0.91616,     0.91616,     0.91539,     0.91508,     0.91474,     0.91451,     0.91451,     0.91379,     0.91222,     0.91189,\n",
              "            0.91117,     0.91084,     0.91046,     0.90938,     0.90905,     0.90872,     0.90851,     0.90851,     0.90782,     0.90545,     0.90206,     0.90206,      0.9017,     0.90111,     0.90111,     0.90044,     0.90013,     0.89797,     0.89797,     0.89731,     0.89705,     0.89675,     0.89658,\n",
              "            0.89658,     0.89554,     0.89524,     0.89455,     0.89387,     0.89293,     0.89293,      0.8923,     0.89047,     0.88994,     0.88994,     0.88932,     0.88903,      0.8878,      0.8878,     0.88709,     0.88573,     0.88545,     0.88489,     0.88387,      0.8822,      0.8822,     0.88065,\n",
              "            0.88065,     0.88007,        0.88,        0.88,        0.88,     0.87937,     0.87811,     0.87707,     0.87614,     0.87552,     0.87356,     0.87295,      0.8727,     0.87173,     0.87082,     0.87022,     0.86099,     0.86006,     0.85874,     0.85618,     0.85267,     0.85239,     0.85116,\n",
              "            0.84573,      0.8442,     0.84346,     0.84346,       0.842,     0.84115,      0.8388,      0.8386,     0.83808,     0.83499,     0.83048,     0.83042,     0.83042,      0.8293,     0.82817,     0.82712,     0.82508,     0.82158,     0.81957,     0.81583,     0.81566,     0.81287,     0.81122,\n",
              "            0.80905,     0.80684,     0.80355,     0.80224,     0.80007,     0.79778,       0.797,     0.78996,     0.78725,      0.7813,      0.7813,     0.77398,     0.77254,     0.76986,     0.76713,     0.76685,     0.76415,     0.76181,     0.76069,     0.75108,        0.75,      0.7445,      0.7411,\n",
              "             0.7391,     0.73577,     0.73381,     0.73147,     0.72652,     0.71833,     0.71738,      0.7114,     0.68438,     0.67819,     0.67479,     0.66938,     0.66455,     0.65021,     0.63067,     0.62847,     0.62251,     0.61657,     0.61313,     0.60914,      0.5983,     0.58835,     0.57301,\n",
              "            0.56846,     0.55248,      0.5475,      0.5386,      0.5322,     0.50518,      0.4843,     0.48133,     0.45969,     0.44292,     0.41219,     0.40007,     0.38426,     0.38019,     0.36009,     0.35936,     0.31927,     0.30893,     0.30389,     0.28259,     0.26328,     0.25289,      0.2317,\n",
              "            0.21478,     0.19878,      0.1897,     0.17565,     0.16208,     0.15649,      0.1509,     0.14531,     0.13973,     0.13414,     0.12855,     0.12296,     0.11737,     0.11178,     0.10619,      0.1006,    0.095014,    0.089425,    0.083835,    0.078246,    0.072657,    0.067068,    0.061479,\n",
              "            0.05589,    0.050301,    0.044712,    0.039123,    0.033534,    0.027945,    0.022356,    0.016767,    0.011178,    0.005589,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.28236,     0.28236,     0.34519,     0.38426,     0.41141,      0.4326,     0.45067,     0.46714,     0.48082,     0.49275,     0.50437,     0.51404,       0.523,     0.53052,     0.53819,     0.54569,     0.55086,     0.55834,     0.56427,     0.56961,     0.57506,     0.57937,     0.58335,\n",
              "            0.58736,     0.59211,     0.59648,     0.60123,     0.60632,     0.61049,     0.61541,     0.61993,     0.62355,     0.62736,     0.63197,     0.63587,     0.63927,     0.64216,     0.64516,     0.64733,     0.65081,     0.65357,     0.65663,     0.65956,     0.66271,     0.66583,      0.6683,\n",
              "            0.67119,     0.67317,     0.67597,     0.67876,     0.68163,     0.68421,     0.68673,     0.68822,     0.69107,     0.69333,     0.69499,     0.69663,     0.69933,     0.70161,     0.70326,     0.70609,     0.70692,     0.70944,      0.7116,     0.71413,     0.71592,     0.71794,     0.72119,\n",
              "            0.72265,     0.72424,     0.72606,     0.72698,      0.7282,      0.7307,     0.73278,     0.73471,       0.737,     0.73928,     0.74111,     0.74242,     0.74375,     0.74552,     0.74636,     0.74811,     0.74939,     0.75028,     0.75112,     0.75212,     0.75317,     0.75401,     0.75551,\n",
              "            0.75676,     0.75803,     0.75951,        0.76,     0.76126,     0.76238,     0.76414,     0.76514,     0.76665,     0.76763,     0.76857,     0.76995,     0.77069,      0.7717,     0.77287,     0.77393,     0.77477,     0.77617,      0.7771,     0.77839,     0.77914,      0.7805,     0.78148,\n",
              "            0.78231,     0.78266,     0.78337,     0.78458,     0.78575,     0.78729,     0.78804,      0.7885,     0.79006,     0.79089,     0.79246,     0.79343,     0.79449,     0.79634,     0.79721,      0.7983,     0.79879,     0.80008,     0.80125,     0.80237,      0.8034,       0.805,     0.80536,\n",
              "            0.80604,      0.8069,     0.80763,     0.80802,     0.80818,     0.80861,     0.80843,     0.80964,     0.81074,     0.81133,     0.81198,     0.81271,     0.81378,     0.81498,     0.81578,     0.81606,     0.81657,     0.81652,     0.81658,     0.81717,     0.81758,     0.81804,     0.81812,\n",
              "             0.8186,     0.81878,     0.81931,     0.82006,     0.82027,       0.821,     0.82172,     0.82223,     0.82289,     0.82385,     0.82432,      0.8247,     0.82525,      0.8257,     0.82591,     0.82669,     0.82683,     0.82727,      0.8275,       0.829,     0.82954,     0.82991,     0.83078,\n",
              "            0.83098,     0.83142,     0.83156,     0.83196,     0.83237,     0.83271,     0.83363,     0.83315,     0.83332,       0.834,     0.83426,     0.83553,     0.83551,     0.83545,     0.83612,     0.83663,     0.83688,     0.83745,     0.83786,     0.83883,     0.83876,     0.83981,     0.83927,\n",
              "             0.8393,     0.84093,     0.84148,     0.84168,     0.84191,     0.84274,     0.84329,     0.84353,     0.84367,     0.84422,      0.8448,     0.84567,     0.84601,     0.84652,     0.84676,     0.84704,      0.8472,     0.84735,      0.8473,     0.84749,     0.84773,     0.84802,     0.84808,\n",
              "            0.84831,     0.84865,     0.84883,     0.84877,     0.84927,     0.84919,     0.84973,     0.85031,     0.85074,     0.85123,     0.85114,     0.85116,     0.85155,     0.85188,     0.85179,     0.85238,     0.85234,     0.85288,     0.85339,     0.85291,     0.85377,     0.85433,     0.85466,\n",
              "            0.85457,     0.85489,     0.85512,     0.85529,     0.85518,     0.85541,      0.8563,     0.85668,      0.8567,     0.85682,     0.85705,      0.8573,     0.85724,     0.85735,     0.85739,     0.85763,     0.85786,     0.85705,     0.85758,     0.85744,     0.85754,     0.85763,     0.85775,\n",
              "            0.85842,     0.85878,     0.85862,     0.85871,       0.859,     0.85936,     0.85992,     0.85964,     0.85962,     0.85936,     0.85921,     0.85993,     0.86002,     0.86014,     0.86003,     0.85989,     0.85988,     0.86035,     0.85967,     0.85988,     0.85986,     0.85955,     0.86038,\n",
              "            0.86039,     0.86094,     0.86218,     0.86208,     0.86221,     0.86252,     0.86246,     0.86218,     0.86252,     0.86353,     0.86327,     0.86338,     0.86376,      0.8645,     0.86431,      0.8645,     0.86462,     0.86499,     0.86538,     0.86557,     0.86609,     0.86675,     0.86714,\n",
              "            0.86745,     0.86756,     0.86781,     0.86788,     0.86835,      0.8685,     0.86836,     0.86841,     0.86826,     0.86814,     0.86805,     0.86779,     0.86796,     0.86782,     0.86774,     0.86773,     0.86805,     0.86807,     0.86804,     0.86791,     0.86783,     0.86769,     0.86804,\n",
              "            0.86815,     0.86812,     0.86797,     0.86788,     0.86772,     0.86732,     0.86671,     0.86642,      0.8662,     0.86601,     0.86548,     0.86554,     0.86542,     0.86528,     0.86577,     0.86566,     0.86554,     0.86518,     0.86482,     0.86493,     0.86461,     0.86479,     0.86476,\n",
              "            0.86464,     0.86404,     0.86434,      0.8641,     0.86404,     0.86365,      0.8629,     0.86281,       0.863,     0.86316,     0.86291,     0.86301,     0.86229,     0.86211,     0.86228,     0.86218,     0.86192,     0.86204,     0.86167,      0.8616,     0.86077,     0.86059,     0.86023,\n",
              "            0.85985,     0.85978,     0.85954,     0.85928,      0.8591,     0.85886,     0.85863,     0.85873,     0.85923,     0.85886,     0.85859,     0.85844,     0.85821,     0.85797,     0.85747,     0.85779,     0.85722,     0.85673,     0.85674,     0.85683,     0.85695,     0.85714,     0.85769,\n",
              "            0.85766,     0.85799,     0.85836,     0.85838,     0.85776,     0.85709,     0.85671,      0.8568,     0.85639,     0.85602,     0.85575,     0.85594,     0.85588,     0.85568,     0.85543,     0.85503,     0.85522,     0.85492,     0.85473,     0.85447,     0.85493,     0.85459,      0.8547,\n",
              "            0.85455,     0.85465,     0.85458,     0.85446,     0.85423,     0.85348,     0.85352,     0.85339,     0.85284,     0.85273,     0.85245,     0.85164,     0.85156,     0.85212,     0.85175,     0.85087,       0.851,     0.85063,        0.85,     0.84824,     0.84722,     0.84752,     0.84722,\n",
              "            0.84734,     0.84749,     0.84697,     0.84647,     0.84651,     0.84671,     0.84654,     0.84571,     0.84525,     0.84512,      0.8446,     0.84376,     0.84369,     0.84291,     0.84265,     0.84162,     0.84129,      0.8407,     0.84029,     0.84047,     0.83963,     0.83947,     0.83869,\n",
              "             0.8384,     0.83826,     0.83824,     0.83779,     0.83738,     0.83723,     0.83696,     0.83577,     0.83505,      0.8349,     0.83482,     0.83471,     0.83368,     0.83334,     0.83254,     0.83175,     0.83088,     0.82982,     0.82954,     0.82866,     0.82824,     0.82798,     0.82817,\n",
              "            0.82776,     0.82764,     0.82769,      0.8275,      0.8267,      0.8268,     0.82671,      0.8259,     0.82536,     0.82461,     0.82441,     0.82333,     0.82244,     0.82027,     0.81828,     0.81692,     0.81592,     0.81501,     0.81419,     0.81354,     0.81263,     0.81188,     0.81088,\n",
              "            0.81078,     0.81041,     0.80985,     0.80912,     0.80883,     0.80789,     0.80734,     0.80622,     0.80501,     0.80389,     0.80267,     0.80126,     0.80088,      0.8005,     0.79955,      0.7987,     0.79718,     0.79633,     0.79518,     0.79423,     0.79409,     0.79306,     0.79277,\n",
              "            0.79181,     0.79066,     0.78979,     0.78943,     0.78894,       0.787,     0.78613,     0.78486,     0.78398,     0.78354,     0.78307,      0.7816,      0.7813,     0.77943,     0.77911,     0.77822,     0.77792,     0.77525,     0.77376,     0.77205,     0.77085,     0.77073,     0.77074,\n",
              "              0.769,     0.76828,     0.76726,     0.76515,     0.76442,     0.76146,     0.76071,     0.75972,     0.75788,     0.75695,     0.75572,     0.75498,       0.753,     0.75238,     0.75206,     0.75174,      0.7512,     0.75006,     0.74756,       0.746,      0.7438,     0.74213,     0.74086,\n",
              "            0.73902,     0.73698,     0.73443,     0.73246,     0.73108,     0.72947,     0.72851,     0.72834,     0.72747,      0.7249,     0.72294,     0.72229,     0.72065,     0.71835,     0.71605,     0.71439,     0.71274,      0.7114,     0.70993,     0.70657,     0.70523,     0.70388,     0.70054,\n",
              "            0.69754,     0.69583,     0.69412,     0.69203,     0.69017,     0.68897,     0.68587,     0.68188,     0.67978,     0.67803,     0.67592,     0.67257,     0.67009,     0.66796,     0.66635,     0.66277,     0.66021,     0.65718,     0.65445,     0.64989,     0.64786,     0.64491,     0.64154,\n",
              "            0.63842,      0.6358,      0.6341,     0.63109,     0.62769,     0.62515,     0.62328,     0.62098,     0.61753,     0.61409,      0.6111,     0.60837,     0.60603,     0.60289,     0.59558,     0.59336,     0.58896,     0.58494,     0.57907,     0.57621,     0.57171,     0.56759,     0.56462,\n",
              "            0.55865,     0.55571,     0.55207,     0.54422,      0.5408,     0.53477,     0.53331,     0.52992,     0.52717,     0.52585,     0.52211,     0.51725,     0.51206,     0.50646,     0.50465,     0.50012,     0.49871,     0.49578,     0.49166,     0.49073,     0.48248,     0.47781,     0.47535,\n",
              "            0.47099,      0.4672,     0.46148,     0.45466,     0.45272,     0.44962,     0.44421,     0.44028,     0.43177,     0.42828,     0.42202,     0.41393,     0.40828,     0.40474,     0.40448,     0.39565,     0.38993,     0.38957,     0.38384,      0.3833,       0.376,     0.37136,       0.371,\n",
              "            0.36599,     0.36571,     0.35843,     0.35106,     0.35069,     0.34479,      0.3362,     0.33562,     0.33065,     0.32461,     0.32422,     0.31801,     0.30918,     0.30908,      0.3032,     0.29521,     0.29491,     0.28392,     0.28351,     0.27472,     0.26704,     0.26663,     0.25704,\n",
              "            0.25045,     0.25002,     0.24034,     0.23425,     0.22938,     0.22895,     0.22156,     0.22123,     0.21395,     0.20292,     0.20247,     0.19612,     0.19038,     0.18993,     0.18422,     0.17907,     0.17861,     0.17358,     0.17101,     0.17054,      0.1638,     0.15893,     0.15524,\n",
              "            0.15477,     0.15153,     0.14607,     0.13873,     0.13825,     0.13072,     0.12455,     0.12406,     0.11857,     0.11083,     0.10652,     0.10602,     0.10188,    0.092579,    0.092075,    0.087874,    0.084399,    0.083891,    0.080166,    0.073403,    0.068608,    0.062784,    0.062265,\n",
              "           0.057787,    0.054858,    0.053567,    0.053042,    0.050037,    0.046338,    0.042709,    0.042179,    0.036174,    0.032267,    0.030945,    0.030409,    0.027976,    0.025929,    0.024488,    0.023002,    0.022461,    0.020332,    0.019651,    0.017388,    0.016028,    0.015255,    0.014928,\n",
              "             0.0146,     0.01333,    0.011931,    0.011168,     0.01062,   0.0098657,   0.0092556,    0.007012,   0.0061864,   0.0054598,   0.0049087,    0.003833,   0.0035964,   0.0033598,   0.0028371,   0.0022608,   0.0019291,   0.0016146,   0.0015356,   0.0014566,   0.0013776,   0.0012985,   0.0012195,\n",
              "          0.0011405,   0.0010614,  0.00098234,  0.00090327,   0.0008242,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.16522,     0.16522,     0.21009,     0.23983,     0.26148,     0.27895,     0.29419,     0.30856,     0.32074,     0.33145,     0.34205,     0.35112,     0.35959,     0.36686,     0.37435,     0.38178,     0.38693,     0.39435,      0.4005,      0.4059,     0.41154,     0.41604,     0.42016,\n",
              "            0.42434,     0.42939,       0.434,     0.43914,     0.44468,     0.44918,     0.45453,     0.45958,     0.46376,     0.46799,     0.47323,     0.47762,     0.48157,     0.48506,     0.48861,      0.4911,     0.49512,     0.49832,     0.50189,     0.50543,     0.50926,     0.51296,     0.51589,\n",
              "            0.51934,     0.52171,     0.52508,     0.52846,     0.53214,     0.53536,     0.53859,     0.54055,      0.5442,     0.54715,      0.5495,     0.55155,     0.55508,     0.55797,     0.56005,     0.56364,     0.56499,     0.56837,      0.5713,     0.57471,     0.57719,     0.57998,     0.58423,\n",
              "            0.58615,      0.5884,     0.59081,     0.59219,     0.59381,     0.59713,     0.60008,     0.60268,      0.6061,     0.60937,     0.61203,       0.614,     0.61616,     0.61877,     0.62004,      0.6227,     0.62466,      0.6259,     0.62716,     0.62882,     0.63049,     0.63203,     0.63414,\n",
              "            0.63591,     0.63769,     0.63979,     0.64049,     0.64228,     0.64388,     0.64639,     0.64782,        0.65,     0.65161,     0.65296,     0.65496,     0.65603,     0.65769,      0.6594,     0.66099,     0.66236,     0.66451,     0.66599,     0.66809,     0.66941,     0.67141,     0.67329,\n",
              "            0.67471,     0.67548,     0.67653,     0.67856,     0.68053,     0.68306,     0.68432,      0.6851,     0.68746,     0.68872,      0.6911,     0.69257,      0.6942,     0.69702,     0.69836,     0.70003,     0.70078,     0.70278,     0.70458,     0.70632,     0.70792,      0.7104,      0.7112,\n",
              "            0.71251,     0.71385,     0.71523,     0.71584,      0.7161,     0.71677,      0.7172,     0.71937,     0.72135,     0.72229,     0.72356,     0.72472,      0.7265,     0.72859,     0.73013,     0.73057,     0.73145,     0.73182,     0.73217,     0.73312,     0.73378,     0.73505,     0.73562,\n",
              "            0.73647,     0.73674,     0.73761,     0.73935,     0.73995,     0.74168,     0.74285,     0.74394,     0.74529,     0.74713,     0.74792,     0.74853,     0.74944,     0.75072,     0.75135,     0.75292,     0.75315,     0.75388,     0.75426,     0.75675,     0.75789,     0.75854,        0.76,\n",
              "            0.76061,     0.76192,     0.76271,     0.76339,     0.76408,     0.76494,     0.76683,     0.76707,     0.76769,     0.76884,     0.76927,     0.77174,     0.77229,     0.77267,     0.77421,     0.77537,     0.77581,     0.77679,      0.7775,      0.7792,     0.77934,     0.78127,     0.78109,\n",
              "            0.78148,     0.78432,     0.78527,     0.78591,     0.78632,     0.78807,     0.78904,     0.78986,     0.79031,     0.79128,      0.7923,     0.79383,     0.79474,     0.79596,     0.79639,     0.79698,     0.79772,     0.79847,     0.79861,     0.79927,     0.79969,     0.80057,     0.80096,\n",
              "            0.80137,      0.8023,     0.80294,     0.80348,     0.80438,     0.80455,     0.80553,     0.80657,       0.808,     0.80888,     0.80905,     0.80942,     0.81012,     0.81105,     0.81116,     0.81262,     0.81306,      0.8142,     0.81546,     0.81594,     0.81751,     0.81853,     0.81949,\n",
              "            0.81967,     0.82026,     0.82067,     0.82133,     0.82217,     0.82259,     0.82424,     0.82505,     0.82533,      0.8259,     0.82632,     0.82707,     0.82773,     0.82828,     0.82867,     0.82986,     0.83041,     0.83044,     0.83193,       0.832,     0.83255,     0.83273,     0.83295,\n",
              "            0.83421,     0.83489,     0.83493,     0.83523,     0.83604,     0.83672,     0.83778,     0.83851,     0.83867,     0.83871,       0.839,     0.84037,     0.84092,     0.84114,     0.84168,     0.84192,     0.84212,      0.8434,     0.84359,       0.844,     0.84417,     0.84409,     0.84572,\n",
              "            0.84725,     0.84832,     0.85073,     0.85132,     0.85157,     0.85218,     0.85264,     0.85306,      0.8541,     0.85616,      0.8561,     0.85658,     0.85733,        0.86,     0.85999,     0.86091,      0.8614,     0.86214,     0.86293,     0.86329,     0.86473,     0.86646,     0.86723,\n",
              "            0.86786,     0.86807,     0.86857,     0.86872,     0.86966,     0.87019,     0.87079,     0.87142,     0.87234,     0.87263,     0.87293,     0.87306,     0.87355,     0.87351,     0.87379,     0.87419,     0.87525,     0.87549,     0.87565,      0.8761,      0.8764,     0.87705,     0.87777,\n",
              "            0.87798,     0.87809,     0.87871,     0.87902,     0.87996,     0.87988,     0.87975,     0.88001,     0.88041,     0.88059,     0.88064,     0.88148,     0.88212,     0.88241,     0.88386,     0.88452,     0.88483,     0.88543,     0.88542,     0.88564,     0.88587,     0.88625,     0.88705,\n",
              "             0.8877,     0.88826,     0.88902,     0.88897,      0.8893,     0.88992,     0.88977,     0.89067,     0.89106,     0.89188,     0.89273,     0.89293,     0.89279,     0.89286,     0.89385,     0.89395,     0.89449,     0.89505,     0.89551,     0.89646,     0.89641,     0.89664,     0.89703,\n",
              "            0.89696,     0.89731,     0.89726,     0.89763,     0.89791,     0.89786,     0.89862,     0.89883,     0.89993,     0.90041,     0.90037,     0.90064,     0.90103,     0.90099,      0.9009,     0.90166,     0.90196,     0.90187,     0.90229,      0.9025,     0.90276,     0.90318,     0.90441,\n",
              "            0.90483,     0.90607,      0.9069,     0.90779,     0.90844,     0.90861,     0.90865,     0.90896,     0.90897,     0.90929,     0.90968,      0.9101,     0.91049,     0.91116,     0.91112,     0.91115,     0.91158,     0.91192,     0.91217,     0.91213,     0.91352,      0.9138,     0.91405,\n",
              "            0.91423,     0.91445,      0.9145,     0.91448,     0.91444,     0.91432,     0.91472,      0.9147,     0.91501,     0.91535,     0.91574,     0.91601,     0.91639,     0.91768,     0.91844,     0.91912,     0.91996,     0.92028,     0.92059,     0.92033,     0.92017,     0.92145,      0.9216,\n",
              "            0.92246,     0.92309,     0.92302,     0.92319,      0.9242,     0.92491,     0.92546,     0.92576,     0.92599,      0.9261,     0.92602,     0.92633,     0.92674,     0.92663,      0.9266,     0.92645,     0.92683,     0.92718,     0.92798,     0.92843,     0.92875,     0.92959,     0.92949,\n",
              "            0.92992,     0.93078,     0.93204,     0.93205,     0.93237,     0.93323,     0.93319,     0.93316,     0.93339,     0.93426,      0.9347,     0.93484,       0.935,     0.93514,     0.93531,     0.93521,     0.93555,     0.93542,     0.93538,     0.93598,     0.93659,     0.93683,     0.93733,\n",
              "             0.9382,     0.93834,     0.93868,     0.94064,     0.94072,     0.94097,     0.94194,     0.94185,     0.94179,     0.94259,     0.94404,     0.94392,      0.9443,     0.94406,     0.94433,     0.94418,     0.94504,     0.94542,     0.94533,     0.94575,     0.94614,     0.94616,     0.94645,\n",
              "            0.94693,     0.94739,     0.94733,     0.94825,     0.94822,      0.9483,     0.94857,     0.94845,     0.94883,     0.94872,      0.9491,     0.94896,     0.94943,      0.9499,     0.95032,     0.95023,      0.9506,     0.95052,      0.9504,     0.95083,     0.95224,     0.95228,     0.95225,\n",
              "            0.95268,     0.95258,     0.95249,     0.95399,     0.95502,     0.95543,     0.95535,     0.95578,      0.9557,     0.95649,     0.95725,     0.95712,     0.95709,     0.95748,     0.95911,     0.95903,     0.95901,     0.95879,     0.95866,     0.95964,     0.95955,      0.9601,     0.96116,\n",
              "             0.9611,     0.96161,      0.9621,     0.96194,     0.96246,      0.9634,     0.96393,     0.96503,      0.9649,     0.96483,     0.96474,     0.96529,     0.96574,      0.9657,     0.96568,     0.96565,     0.96609,     0.96614,     0.96596,     0.96585,      0.9657,     0.96681,     0.96672,\n",
              "            0.96784,     0.96833,     0.96816,     0.96837,     0.96857,     0.96847,     0.97032,     0.97031,     0.97155,     0.97205,     0.97193,     0.97189,      0.9718,     0.97166,     0.97152,     0.97142,     0.97132,     0.97124,     0.97115,     0.97094,     0.97086,     0.97078,     0.97194,\n",
              "            0.97318,     0.97445,     0.97436,     0.97424,     0.97459,     0.97478,     0.97461,     0.97512,       0.975,     0.97491,     0.97479,     0.97461,     0.97447,     0.97435,     0.97501,     0.97481,     0.97534,     0.97604,     0.97667,     0.97722,      0.9779,     0.97776,     0.97759,\n",
              "            0.97743,      0.9773,     0.97803,     0.97788,     0.97771,     0.97862,     0.97916,     0.97905,     0.97889,     0.97958,     0.98031,     0.98018,     0.98007,     0.97993,     0.97958,     0.98038,     0.98018,     0.97999,     0.97971,     0.97957,     0.97935,     0.97914,     0.97899,\n",
              "            0.97869,     0.97853,     0.97934,     0.97894,     0.97876,     0.97844,     0.97836,     0.97924,     0.98124,     0.98118,       0.981,     0.98076,     0.98162,     0.98135,     0.98126,     0.98104,     0.98097,     0.98082,     0.98061,     0.98057,     0.98256,     0.98234,     0.98222,\n",
              "            0.98201,     0.98182,     0.98153,      0.9825,      0.9824,     0.98225,     0.98197,     0.98177,     0.98272,     0.98255,     0.98222,     0.98179,     0.98148,      0.9828,     0.98279,     0.98389,      0.9836,     0.98359,     0.98329,     0.98326,     0.98286,      0.9826,     0.98258,\n",
              "            0.98403,     0.98402,     0.98541,     0.98688,     0.98687,     0.98971,     0.99009,     0.99007,     0.98989,     0.98967,     0.98965,     0.98942,     0.98937,     0.99122,     0.99102,     0.99074,     0.99073,     0.99031,     0.99029,     0.98994,      0.9896,     0.98959,     0.98914,\n",
              "            0.98882,      0.9888,     0.99119,     0.99093,     0.99071,     0.99069,     0.99034,     0.99033,     0.98996,     0.98936,     0.98933,     0.98895,     0.98859,     0.98856,     0.99208,     0.99183,     0.99181,     0.99155,     0.99569,     0.99567,     0.99548,     0.99533,     0.99521,\n",
              "            0.99519,     0.99508,     0.99488,     0.99459,     0.99457,     0.99424,     0.99394,     0.99391,     0.99361,     0.99314,     0.99285,     0.99281,     0.99251,     0.99172,     0.99167,     0.99126,     0.99089,     0.99083,     0.99039,     0.98948,     0.98872,     0.98765,     0.98755,\n",
              "            0.98656,     0.98584,     0.98549,     0.98534,     0.98445,      0.9832,     0.98177,     0.98154,     0.97847,     0.97588,     0.97486,     0.97443,     0.99639,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.97041,     0.97041,     0.96717,     0.96595,     0.96433,     0.96311,     0.96271,     0.96109,     0.95987,     0.95987,     0.95987,     0.95906,     0.95865,     0.95784,     0.95703,     0.95622,     0.95582,     0.95582,      0.9546,      0.9546,      0.9542,     0.95379,     0.95379,\n",
              "            0.95379,     0.95338,     0.95338,     0.95298,     0.95257,     0.95257,     0.95257,     0.95217,     0.95136,     0.95136,     0.95095,     0.95095,     0.95055,     0.94974,     0.94933,     0.94933,     0.94933,     0.94933,     0.94933,     0.94893,     0.94852,     0.94852,     0.94852,\n",
              "            0.94852,     0.94852,     0.94852,     0.94852,     0.94791,     0.94771,     0.94727,      0.9469,     0.94649,     0.94609,     0.94528,     0.94528,     0.94487,     0.94487,     0.94487,     0.94487,     0.94406,     0.94366,     0.94325,     0.94285,     0.94244,     0.94203,     0.94203,\n",
              "            0.94203,     0.94163,     0.94163,     0.94122,     0.94122,     0.94122,     0.94082,     0.94082,     0.94001,      0.9396,      0.9392,     0.93879,     0.93798,     0.93758,     0.93732,     0.93677,     0.93636,     0.93636,     0.93617,     0.93555,     0.93514,     0.93433,     0.93433,\n",
              "            0.93433,     0.93433,     0.93433,     0.93433,     0.93433,     0.93433,     0.93433,     0.93433,     0.93433,     0.93393,     0.93393,     0.93393,     0.93393,     0.93352,     0.93352,     0.93341,     0.93312,     0.93294,     0.93271,     0.93231,      0.9319,      0.9319,     0.93109,\n",
              "            0.93074,     0.93028,     0.93028,     0.92987,     0.92947,     0.92906,     0.92881,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92866,     0.92825,\n",
              "            0.92785,     0.92785,     0.92744,     0.92744,     0.92744,     0.92744,     0.92624,     0.92582,     0.92542,     0.92542,     0.92501,     0.92501,     0.92491,      0.9246,      0.9242,      0.9242,     0.92412,     0.92339,     0.92298,     0.92298,     0.92298,     0.92217,     0.92146,\n",
              "            0.92136,     0.92136,     0.92136,     0.92055,     0.92015,     0.91934,     0.91934,     0.91893,     0.91852,     0.91812,     0.91812,     0.91812,     0.91812,     0.91731,      0.9169,      0.9165,      0.9165,      0.9165,      0.9165,      0.9165,     0.91615,     0.91609,     0.91609,\n",
              "             0.9157,     0.91488,     0.91407,     0.91407,     0.91407,     0.91366,     0.91317,      0.9117,     0.91123,     0.91123,     0.91123,     0.91082,     0.91001,     0.90933,      0.9088,     0.90839,     0.90839,     0.90839,     0.90839,     0.90834,     0.90799,     0.90782,     0.90682,\n",
              "            0.90636,     0.90636,     0.90636,     0.90596,     0.90596,     0.90555,     0.90555,     0.90502,     0.90474,     0.90474,     0.90474,     0.90474,     0.90434,     0.90393,     0.90393,     0.90381,     0.90321,     0.90259,     0.90231,     0.90191,     0.90191,     0.90146,     0.90109,\n",
              "            0.90109,     0.90069,     0.90028,     0.89947,     0.89947,     0.89907,     0.89907,     0.89907,     0.89826,     0.89826,     0.89785,     0.89745,     0.89745,     0.89704,     0.89671,     0.89623,     0.89561,     0.89542,     0.89501,     0.89339,     0.89339,     0.89339,     0.89299,\n",
              "            0.89258,     0.89258,     0.89258,     0.89218,     0.89096,     0.89096,     0.89096,     0.89082,     0.89056,     0.89015,     0.89015,     0.88982,     0.88893,     0.88853,     0.88816,     0.88731,     0.88719,     0.88542,     0.88486,     0.88448,     0.88407,     0.88407,     0.88407,\n",
              "            0.88407,     0.88407,     0.88369,     0.88354,     0.88326,     0.88326,     0.88326,     0.88186,     0.88164,     0.88106,     0.88042,     0.88042,     0.88002,     0.88002,     0.87921,     0.87865,     0.87839,     0.87799,     0.87637,     0.87637,     0.87615,     0.87559,     0.87556,\n",
              "            0.87394,     0.87394,     0.87394,     0.87313,     0.87313,     0.87313,     0.87251,      0.8715,      0.8711,     0.87101,     0.87057,     0.87029,     0.87029,     0.86905,     0.86867,     0.86812,     0.86786,     0.86786,     0.86786,     0.86786,     0.86745,     0.86704,     0.86704,\n",
              "            0.86704,     0.86704,     0.86704,     0.86704,     0.86704,     0.86681,     0.86595,     0.86542,     0.86421,     0.86369,     0.86323,     0.86259,     0.86245,     0.86221,     0.86178,     0.86137,     0.86096,     0.86077,     0.86056,     0.85987,     0.85942,     0.85853,     0.85853,\n",
              "            0.85853,     0.85837,     0.85749,     0.85703,      0.8558,     0.85511,     0.85406,     0.85323,     0.85245,      0.8519,     0.85083,     0.85016,     0.84934,      0.8488,      0.8484,     0.84759,     0.84707,     0.84584,     0.84516,     0.84516,     0.84435,     0.84435,     0.84357,\n",
              "            0.84274,      0.8411,       0.841,     0.84058,     0.84017,      0.8389,     0.83761,     0.83664,     0.83664,     0.83624,     0.83502,     0.83502,      0.8338,      0.8334,     0.83287,     0.83259,     0.83163,     0.83137,      0.8303,     0.82935,     0.82784,     0.82732,     0.82633,\n",
              "            0.82569,     0.82527,     0.82485,     0.82408,     0.82352,      0.8231,     0.82205,     0.82205,     0.82205,     0.82096,     0.82052,     0.82002,     0.81928,     0.81886,     0.81804,       0.818,     0.81671,     0.81589,     0.81557,     0.81557,     0.81557,     0.81557,     0.81557,\n",
              "            0.81516,     0.81475,     0.81475,     0.81407,     0.81244,     0.81111,     0.81039,      0.8103,     0.80956,     0.80864,     0.80786,     0.80786,     0.80746,     0.80657,     0.80616,     0.80543,     0.80543,     0.80462,     0.80409,     0.80367,      0.8034,     0.80259,     0.80259,\n",
              "            0.80219,     0.80219,     0.80204,     0.80183,     0.80146,     0.80024,     0.79999,     0.79978,     0.79859,     0.79814,     0.79735,     0.79572,      0.7953,      0.7953,     0.79408,     0.79206,     0.79165,     0.79078,     0.78947,     0.78662,     0.78499,     0.78457,     0.78395,\n",
              "            0.78354,     0.78333,      0.7825,     0.78152,     0.78086,     0.78071,     0.78003,      0.7784,     0.77746,     0.77716,     0.77634,     0.77471,      0.7743,     0.77307,     0.77265,     0.77102,      0.7702,     0.76898,     0.76775,     0.76773,     0.76611,     0.76529,     0.76406,\n",
              "            0.76328,     0.76246,      0.7616,     0.76084,     0.75996,     0.75914,     0.75872,     0.75679,     0.75546,     0.75464,     0.75422,     0.75395,     0.75217,     0.75152,     0.75013,      0.7489,     0.74727,     0.74564,     0.74522,     0.74341,     0.74236,     0.74179,     0.74179,\n",
              "            0.74058,      0.7403,     0.74017,     0.73866,     0.73733,     0.73733,      0.7366,     0.73537,     0.73455,     0.73287,     0.73169,     0.73006,     0.72843,     0.72518,     0.72192,     0.71989,     0.71785,     0.71622,     0.71499,     0.71377,     0.71213,     0.71099,     0.70928,\n",
              "            0.70886,     0.70804,     0.70722,     0.70559,     0.70517,     0.70369,     0.70272,     0.70109,     0.69905,     0.69742,     0.69538,     0.69335,     0.69253,     0.69171,     0.69007,     0.68885,     0.68641,     0.68518,     0.68355,     0.68192,     0.68099,     0.67946,     0.67905,\n",
              "            0.67742,     0.67578,     0.67456,     0.67329,     0.67207,     0.66906,     0.66783,     0.66579,     0.66457,     0.66356,     0.66252,     0.66048,     0.66007,     0.65722,     0.65599,     0.65477,     0.65435,     0.65069,     0.64866,     0.64581,     0.64418,     0.64376,     0.64329,\n",
              "             0.6409,     0.63968,     0.63805,      0.6352,     0.63397,      0.6295,     0.62826,     0.62644,       0.624,     0.62277,     0.62114,     0.61992,     0.61707,     0.61625,     0.61583,     0.61542,     0.61451,     0.61297,     0.60971,     0.60768,     0.60483,     0.60218,     0.60055,\n",
              "             0.5977,     0.59486,      0.5916,     0.58897,     0.58712,     0.58509,     0.58317,     0.58297,     0.58141,     0.57795,     0.57551,     0.57469,     0.57265,     0.56981,     0.56696,     0.56492,     0.56288,     0.56125,     0.55945,     0.55535,     0.55372,     0.55209,     0.54762,\n",
              "            0.54358,     0.54112,     0.53908,     0.53659,     0.53425,     0.53277,     0.52911,     0.52423,     0.52179,     0.51975,     0.51731,     0.51345,      0.5106,     0.50816,     0.50612,     0.50206,     0.49899,     0.49535,      0.4921,     0.48682,     0.48438,     0.48112,     0.47743,\n",
              "            0.47401,     0.47116,     0.46913,     0.46587,     0.46221,     0.45926,     0.45713,     0.45469,     0.45103,     0.44723,     0.44391,     0.44107,     0.43862,     0.43537,     0.42786,     0.42542,     0.42095,     0.41689,       0.411,     0.40815,     0.40368,     0.39962,      0.3967,\n",
              "            0.39088,     0.38804,     0.38438,     0.37687,     0.37362,     0.36793,     0.36657,     0.36325,      0.3604,     0.35917,     0.35572,     0.35125,     0.34638,      0.3413,     0.33967,      0.3356,     0.33434,     0.33173,     0.32807,     0.32725,     0.31974,     0.31568,     0.31355,\n",
              "            0.30978,     0.30653,     0.30165,     0.29577,     0.29414,     0.29153,     0.28702,     0.28377,     0.27666,     0.27382,     0.26874,     0.26224,     0.25775,     0.25485,     0.25464,     0.24761,     0.24317,     0.24289,     0.23846,     0.23805,     0.23246,     0.22894,     0.22867,\n",
              "             0.2248,     0.22459,     0.21905,     0.21351,     0.21323,     0.20876,     0.20248,     0.20206,     0.19848,     0.19414,     0.19387,     0.18945,     0.18322,     0.18308,     0.17898,     0.17345,     0.17324,     0.16572,     0.16544,     0.15949,     0.15435,     0.15407,     0.14771,\n",
              "            0.14338,      0.1431,     0.13675,     0.13282,     0.12971,     0.12943,     0.12473,     0.12453,     0.11994,     0.11305,     0.11278,     0.10885,     0.10533,     0.10506,     0.10154,    0.098421,    0.098144,    0.095118,    0.093536,    0.093259,    0.089243,    0.086362,    0.084186,\n",
              "           0.083909,    0.082011,    0.078823,    0.074566,    0.074289,    0.069958,    0.066439,    0.066162,    0.063047,    0.058687,    0.056278,    0.056001,    0.053697,    0.048556,    0.048279,    0.045975,    0.044077,      0.0438,    0.041774,    0.038115,    0.035537,    0.032423,    0.032146,\n",
              "           0.029765,    0.028214,    0.027532,    0.027255,    0.025671,    0.023728,     0.02183,    0.021553,    0.018427,    0.016405,    0.015722,    0.015445,    0.014187,    0.013135,    0.012396,    0.011635,    0.011358,     0.01027,    0.009923,   0.0087704,   0.0080786,   0.0076862,     0.00752,\n",
              "          0.0073538,   0.0067099,   0.0060011,   0.0056154,   0.0053384,   0.0049573,   0.0046493,   0.0035183,   0.0031028,   0.0027374,   0.0024604,   0.0019202,   0.0018014,   0.0016827,   0.0014206,   0.0011317,  0.00096546,  0.00080798,  0.00076841,  0.00072884,  0.00068926,  0.00064969,  0.00061012,\n",
              "         0.00057055,  0.00053098,  0.00049141,  0.00045184,  0.00041227,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.5338338116643049)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.49195])\n",
              "names: {0: 'person'}\n",
              "nt_per_class: array([2467])\n",
              "nt_per_image: array([352])\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.8756475729923384), 'metrics/recall(B)': np.float64(0.860559383867045), 'metrics/mAP50(B)': np.float64(0.9107514522721446), 'metrics/mAP50-95(B)': np.float64(0.49195407381898937), 'fitness': np.float64(0.5338338116643049)}\n",
              "save_dir: PosixPath('/content/drive/MyDrive/runs/detect/train4')\n",
              "speed: {'preprocess': 0.3555306964635588, 'inference': 24.318459017542825, 'loss': 0.0009814947445288692, 'postprocess': 1.9024510333302285}\n",
              "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# training the model\n",
        "yolo_checkpoint = os.path.join(PATH, 'train4/weights/last.pt')\n",
        "\n",
        "if os.path.exists(yolo_checkpoint):\n",
        "    print(\"âœ… Resuming YOLOv12-L training...\")\n",
        "    model_path = yolo_checkpoint\n",
        "    resume_training = True\n",
        "else:\n",
        "    print(\"ğŸš€ Starting new YOLOv12-L training...\")\n",
        "    model_path = 'yolo12l.pt'\n",
        "    resume_training = False\n",
        "\n",
        "modelYOLO = YOLO(model_path)\n",
        "modelYOLO.train(\n",
        "    resume=resume_training,\n",
        "    project=PATH,\n",
        "    name='train4',\n",
        "    data=locDataset,\n",
        "    epochs=50,\n",
        "    batch=8,\n",
        "    lr0=0.0001,  # Sets the initial learning rate\n",
        "    imgsz=640\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsPa3Nd8HJQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b2170f-45cb-4e93-afff-5a68f92e3741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting new RT-DETR-L training...\n",
            "Ultralytics 8.3.167 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Human-Thermal-Detection-5/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=rtdetr-l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train32, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/runs/detect/train32, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "WARNING âš ï¸ no model scale passed. Assuming scale='l'.\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
            "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
            "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
            "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
            "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
            "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
            "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
            "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
            "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
            "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
            " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
            " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
            " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
            " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
            " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
            " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
            " 28        [21, 24, 27]  1   7303907  ultralytics.nn.modules.head.RTDETRDecoder    [1, [256, 256, 256]]          \n",
            "rt-detr-l summary: 457 layers, 32,808,131 parameters, 32,808,131 gradients, 108.0 GFLOPs\n",
            "\n",
            "Transferred 926/941 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1647.2Â±728.5 MB/s, size: 62.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Human-Thermal-Detection-5/train/labels.cache... 4016 images, 1689 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4016/4016 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 398.7Â±170.7 MB/s, size: 32.8 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Human-Thermal-Detection-5/valid/labels.cache... 570 images, 218 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/drive/MyDrive/runs/detect/train32/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.0005), 226 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/runs/detect/train32\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       1/50      7.05G     0.8841     0.7469     0.1307         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:27<00:00,  1.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.742      0.716      0.732      0.303\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       2/50      7.14G     0.7405     0.5356    0.07487         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:19<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.821      0.784      0.843      0.387\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       3/50      7.14G     0.7491     0.5484    0.07512         78        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:18<00:00,  1.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.778      0.771      0.815      0.353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       4/50      7.21G     0.7108     0.5143    0.06635         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:14<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.834      0.807      0.866        0.4\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       5/50      7.23G     0.7056     0.5114    0.06768         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:16<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467       0.85      0.824      0.882      0.416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       6/50      7.23G     0.7216     0.5389    0.07218         66        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:15<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.781      0.773      0.802      0.358\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       7/50      7.23G     0.7137     0.5206    0.07051         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467       0.82      0.792      0.855      0.411\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       8/50      7.24G     0.6861     0.5065    0.06506         67        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.832      0.816      0.861       0.41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "       9/50      7.24G     0.6904     0.5274    0.06629         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.791      0.777      0.807      0.389\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      10/50      7.24G     0.6754     0.5045     0.0639         65        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:11<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.843      0.849      0.874      0.437\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      11/50      7.24G     0.6652     0.5003    0.06105         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:15<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.841      0.823      0.881       0.43\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      12/50      7.24G     0.6575     0.5009    0.05965         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.869      0.813      0.873      0.422\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      13/50      7.24G     0.6457     0.4883    0.05935         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:11<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.852      0.834      0.884      0.441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      14/50      7.24G     0.6737     0.5155    0.06396         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:15<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.872      0.848      0.904      0.446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      15/50      7.24G     0.6512     0.4894    0.05878         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:16<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467       0.85      0.848      0.874      0.443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      16/50      7.24G     0.6484     0.4865    0.05813         64        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:14<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.874      0.854      0.906      0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      17/50      7.24G      0.645     0.4824    0.05844         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:17<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.871      0.861      0.899      0.449\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      18/50      7.24G     0.6433     0.4942    0.05931         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.854      0.848      0.889      0.446\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      19/50      7.24G     0.6353     0.4785    0.05695         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.875      0.862      0.902      0.453\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      20/50      7.24G     0.6253     0.4822    0.05512         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.871      0.869      0.913      0.465\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      21/50      7.24G     0.6196      0.478    0.05424         72        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:16<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.881      0.862      0.902       0.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      22/50      7.24G     0.6047      0.475    0.05365         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.871      0.853      0.896      0.451\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      23/50      7.24G      0.602     0.4756    0.05229         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:14<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.888      0.878      0.917      0.478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      24/50      7.24G     0.6071     0.4678    0.05371         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:17<00:00,  1.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.868      0.864      0.904      0.465\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      25/50      7.24G     0.6143      0.474    0.05607         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.882      0.859      0.907       0.47\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      26/50      7.24G     0.6083     0.4705    0.05335         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.882      0.871      0.912      0.473\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      27/50      7.24G     0.6105     0.4757    0.05318         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.882       0.87      0.916      0.484\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      28/50      7.24G      0.598     0.4687    0.05191         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:14<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.879      0.878      0.919      0.484\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      29/50      7.24G     0.6156     0.5463    0.05596         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:16<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.837      0.851      0.871      0.438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      30/50      7.24G     0.6069     0.4895    0.05437         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:14<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.872      0.859      0.903      0.464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      31/50      7.24G     0.6072     0.4848    0.05461         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.867      0.878      0.911      0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      32/50      7.24G     0.5988     0.4724    0.05256         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.877      0.896      0.918      0.486\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      33/50      7.24G     0.5856     0.4672    0.05083         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:15<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.896      0.884      0.921      0.489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      34/50      7.24G     0.5869     0.4677    0.05173         59        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:15<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.883       0.88      0.913      0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      35/50      7.24G     0.5764     0.4644    0.05051         76        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.881      0.897      0.921      0.498\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      36/50      7.24G     0.5802     0.4629    0.04994         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:14<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.899      0.883      0.922      0.495\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      37/50      7.24G     0.5772     0.4628    0.05026         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.893      0.897      0.925      0.493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      38/50      7.24G     0.5773     0.4595    0.04988         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:13<00:00,  1.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.902      0.899      0.933      0.503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      39/50      7.24G     0.5719     0.4596    0.04867         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:15<00:00,  1.59it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.903      0.851      0.897      0.484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      40/50      7.24G      0.578     0.4606    0.05032         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.888      0.889      0.927      0.498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      41/50      7.24G       0.54     0.4676    0.05248          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:10<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.893      0.872       0.92      0.488\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      42/50      7.24G     0.5421     0.4699     0.0526         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:10<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.885      0.875      0.906      0.487\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      43/50      7.24G     0.5327     0.4554    0.05231         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:11<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.884      0.882       0.91      0.495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      44/50      7.24G     0.5367     0.4534    0.05223         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:10<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.899      0.898      0.931        0.5\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      45/50      7.24G      0.526     0.4493    0.05093         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:10<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.872      0.789      0.812      0.435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      46/50      7.24G      0.517     0.4616    0.04968         76        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:09<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.887      0.861      0.888      0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      47/50      7.24G     0.5169     0.4478    0.04983         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:09<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.907      0.892      0.932       0.51\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      48/50      7.24G     0.5084     0.4398    0.04913         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.897      0.906      0.933      0.509\n",
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      49/50      7.24G     0.5056      0.438    0.04903         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:12<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.905      0.899      0.933      0.508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/502 [00:00<?, ?it/s]grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:92.)\n",
            "      50/50      7.24G     0.5074     0.4394    0.04929         54        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [05:11<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.897      0.904      0.935      0.514\n",
            "\n",
            "50 epochs completed in 4.553 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/runs/detect/train32/weights/last.pt, 66.1MB\n",
            "Optimizer stripped from /content/drive/MyDrive/runs/detect/train32/weights/best.pt, 66.1MB\n",
            "\n",
            "Validating /content/drive/MyDrive/runs/detect/train32/weights/best.pt...\n",
            "Ultralytics 8.3.167 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "rt-detr-l summary: 302 layers, 31,985,795 parameters, 0 gradients, 103.4 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:12<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        570       2467      0.897      0.904      0.934      0.514\n",
            "Speed: 0.3ms preprocess, 16.5ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/runs/detect/train32\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([0])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7bf7211fc850>\n",
              "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
              "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,\n",
              "             0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,\n",
              "             0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,\n",
              "             0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,\n",
              "             0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,      0.9939,     0.99118,     0.99118,     0.99118,     0.99118,      0.9898,\n",
              "             0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,      0.9898,     0.98848,     0.98848,     0.98848,\n",
              "            0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,\n",
              "            0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,     0.98848,\n",
              "            0.98848,     0.98848,     0.98677,     0.98677,     0.98677,     0.98545,     0.98545,     0.98545,     0.98545,     0.98545,     0.98545,     0.98545,     0.98545,     0.98459,     0.98459,     0.98459,     0.98459,     0.98459,     0.98459,     0.98459,     0.98459,     0.98459,     0.98459,\n",
              "            0.98459,     0.98459,     0.98459,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,\n",
              "            0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,\n",
              "            0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,\n",
              "            0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,\n",
              "            0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,     0.98337,      0.9827,      0.9827,      0.9827,      0.9827,      0.9827,      0.9827,      0.9827,      0.9827,      0.9827,\n",
              "             0.9827,      0.9822,      0.9822,      0.9822,      0.9822,      0.9822,      0.9822,      0.9822,      0.9822,      0.9822,      0.9822,      0.9822,      0.9822,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,\n",
              "            0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,     0.98171,\n",
              "            0.98171,     0.98117,     0.98117,     0.98117,     0.98117,     0.98117,     0.98117,     0.98117,     0.98117,     0.98117,     0.98043,     0.98043,     0.98043,     0.98043,     0.98043,     0.97983,     0.97983,     0.97983,     0.97983,     0.97983,     0.97983,     0.97983,     0.97983,\n",
              "            0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,\n",
              "            0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,\n",
              "            0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,\n",
              "            0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97953,     0.97909,     0.97909,\n",
              "            0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,\n",
              "            0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97909,     0.97843,     0.97843,\n",
              "             0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,      0.9781,\n",
              "             0.9781,      0.9781,      0.9781,      0.9781,     0.97752,     0.97752,     0.97752,      0.9763,      0.9763,     0.97527,     0.97527,     0.97527,     0.97527,     0.97527,     0.97527,     0.97527,     0.97475,     0.97475,     0.97475,     0.97475,     0.97427,     0.97427,     0.97427,\n",
              "            0.97427,     0.97332,     0.97332,     0.97332,     0.97332,     0.97332,     0.97332,     0.97332,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97323,     0.97265,     0.97166,\n",
              "            0.97166,     0.97166,     0.97166,     0.97166,     0.97118,     0.97118,     0.97118,     0.97073,     0.97073,     0.97073,     0.97073,     0.96964,     0.96964,     0.96921,     0.96921,     0.96921,     0.96921,     0.96877,     0.96877,     0.96877,     0.96837,     0.96837,     0.96837,\n",
              "            0.96837,     0.96782,     0.96733,     0.96733,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,     0.96596,     0.96596,\n",
              "            0.96552,     0.96552,     0.96514,     0.96514,     0.96514,     0.96514,     0.96496,     0.96496,     0.96496,     0.96496,     0.96496,     0.96496,     0.96496,     0.96496,     0.96463,     0.96463,     0.96463,     0.96463,     0.96431,     0.96431,     0.96431,     0.96431,     0.96431,\n",
              "            0.96388,     0.96388,     0.96346,     0.96346,     0.96346,     0.96265,     0.96265,     0.96265,     0.96265,     0.96265,     0.96167,     0.96143,     0.96143,     0.96143,     0.96143,     0.96143,     0.96143,     0.96143,     0.96143,     0.96143,     0.96143,     0.96143,     0.96143,\n",
              "            0.96143,     0.96143,     0.96143,     0.96143,     0.96119,     0.96119,     0.96119,     0.96119,     0.96119,     0.96119,     0.96073,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,\n",
              "            0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.96063,     0.95923,     0.95923,     0.95812,     0.95812,     0.95812,     0.95812,     0.95812,     0.95812,     0.95812,     0.95619,     0.95532,     0.95532,     0.95522,     0.95522,     0.95522,     0.95522,     0.95522,\n",
              "            0.95522,     0.95522,     0.95436,     0.95436,     0.95409,     0.95409,     0.95409,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,     0.95401,\n",
              "            0.95363,     0.95327,     0.95327,     0.95166,     0.95166,     0.95166,     0.95166,     0.95166,     0.95134,     0.95134,      0.9509,     0.95056,     0.95056,     0.95039,     0.95039,     0.95039,     0.95039,     0.94962,     0.94962,     0.94962,     0.94935,     0.94935,     0.94935,\n",
              "            0.94893,     0.94812,     0.94772,     0.94734,     0.94714,     0.94714,     0.94714,     0.94714,     0.94687,     0.94687,     0.94687,     0.94603,     0.94525,     0.94447,     0.94447,     0.94426,     0.94426,     0.94426,     0.94426,     0.94327,     0.94327,     0.94327,     0.94327,\n",
              "            0.94327,     0.94296,     0.94296,      0.9422,     0.94185,     0.94104,     0.94064,     0.94032,     0.93995,     0.93971,     0.93971,     0.93971,     0.93934,     0.93854,     0.93742,     0.93742,     0.93688,     0.93688,     0.93688,     0.93688,     0.93672,     0.93672,     0.93672,\n",
              "            0.93672,     0.93594,     0.93564,     0.93525,     0.93457,     0.93457,     0.93398,     0.93398,     0.93398,      0.9333,      0.9333,     0.93254,     0.93063,     0.93028,     0.92994,     0.92839,     0.92687,     0.92537,      0.9251,      0.9251,      0.9244,     0.92327,     0.92232,\n",
              "            0.92232,     0.92232,     0.92046,     0.92024,     0.92024,     0.91914,     0.91846,      0.9174,     0.91709,     0.91523,     0.91423,     0.91392,     0.91357,     0.91254,     0.91083,     0.91083,     0.90939,     0.90879,     0.90815,     0.90782,     0.90463,     0.90392,     0.90375,\n",
              "            0.90375,     0.90236,     0.90175,     0.90073,     0.90045,     0.89847,     0.89847,     0.89611,     0.89516,     0.89377,     0.89287,      0.8922,     0.89052,     0.89034,     0.89003,     0.88841,      0.8864,     0.88623,     0.88593,     0.88339,     0.88339,     0.88176,     0.87645,\n",
              "            0.87428,     0.87299,     0.87041,     0.86331,     0.86178,     0.85838,     0.85591,     0.85193,     0.85083,     0.85031,     0.84948,     0.84777,     0.84477,     0.83666,     0.83526,     0.83327,     0.83201,     0.83028,     0.82898,     0.82834,      0.8281,     0.82441,     0.81421,\n",
              "            0.80812,     0.80248,     0.79415,     0.78873,     0.78622,     0.77781,     0.77055,     0.76367,     0.75709,     0.75571,     0.74635,     0.73236,      0.7155,     0.69646,     0.68761,     0.67504,     0.62738,     0.61425,     0.59464,     0.59002,     0.54879,     0.53494,     0.50084,\n",
              "            0.49014,     0.47268,     0.42028,     0.37326,     0.35465,     0.33058,     0.28244,     0.20673,     0.19675,     0.15481,     0.14318,     0.13632,     0.11028,    0.067382,    0.061146,    0.052876,    0.044684,    0.042736,     0.04179,    0.037229,    0.031118,    0.023087,    0.013378,\n",
              "           0.012162,    0.010946,   0.0097296,   0.0085134,   0.0072972,    0.006081,   0.0048648,   0.0036486,   0.0024324,   0.0012162,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.028158,    0.028158,    0.028158,    0.028158,    0.028158,    0.028158,    0.028158,     0.05633,    0.080496,    0.094146,     0.10638,     0.11744,     0.12829,     0.13808,     0.14852,     0.15802,     0.16741,     0.17749,     0.18674,     0.19642,     0.20575,     0.21481,     0.22408,\n",
              "             0.2336,     0.24302,     0.25163,     0.26136,     0.27065,     0.27958,      0.2889,     0.29828,     0.30857,     0.31725,     0.32649,     0.33562,     0.34406,     0.35301,     0.36093,     0.37017,     0.37752,     0.38575,     0.39311,     0.40195,     0.41029,     0.41802,     0.42523,\n",
              "            0.43243,     0.43946,     0.44664,      0.4536,     0.46136,     0.46841,     0.47591,     0.48267,     0.48962,     0.49527,       0.502,     0.50687,     0.51313,     0.51747,     0.52308,     0.52921,      0.5344,     0.53912,     0.54449,     0.54878,     0.55388,     0.55933,     0.56502,\n",
              "            0.56917,     0.57393,     0.57908,     0.58324,     0.58752,      0.5914,     0.59601,     0.59948,     0.60442,      0.6096,     0.61321,     0.61747,      0.6215,     0.62588,     0.62922,     0.63261,     0.63687,     0.64042,     0.64365,     0.64753,     0.65039,     0.65325,     0.65731,\n",
              "            0.66041,      0.6641,     0.66756,     0.67122,     0.67436,     0.67769,     0.68029,     0.68261,     0.68504,     0.68827,     0.69085,     0.69514,     0.69763,     0.69943,     0.70187,     0.70337,     0.70591,     0.70889,     0.71251,     0.71405,     0.71633,     0.71864,     0.72027,\n",
              "            0.72202,     0.72401,     0.72567,     0.72744,     0.73024,     0.73261,     0.73469,       0.736,     0.73763,      0.7399,     0.74148,     0.74271,     0.74459,     0.74649,     0.74793,     0.74907,     0.75026,     0.75215,      0.7536,     0.75588,     0.75748,     0.75935,     0.76086,\n",
              "             0.7621,     0.76422,      0.7661,     0.76804,     0.76973,     0.77059,     0.77182,      0.7734,     0.77495,     0.77612,     0.77741,     0.77937,     0.78003,     0.78165,      0.7829,     0.78412,     0.78518,     0.78703,     0.78914,     0.78998,     0.79142,     0.79242,     0.79383,\n",
              "             0.7945,     0.79579,     0.79654,     0.79753,     0.79813,     0.79882,     0.80003,      0.8007,      0.8009,     0.80265,     0.80409,      0.8047,     0.80546,     0.80617,     0.80664,     0.80791,     0.80849,     0.80991,     0.81069,     0.81198,     0.81306,     0.81383,     0.81549,\n",
              "            0.81624,     0.81704,     0.81762,     0.81843,     0.81876,     0.81953,      0.8203,       0.821,     0.82208,     0.82282,     0.82427,     0.82508,     0.82653,     0.82735,     0.82766,     0.82843,     0.82899,     0.82938,     0.82963,      0.8302,     0.83078,     0.83122,     0.83192,\n",
              "            0.83339,     0.83365,     0.83428,       0.835,      0.8351,     0.83542,     0.83617,     0.83662,     0.83705,     0.83735,     0.83756,     0.83817,      0.8385,     0.83892,     0.83908,     0.83939,      0.8396,     0.83951,      0.8404,     0.84094,     0.84161,     0.84188,     0.84226,\n",
              "            0.84258,     0.84283,     0.84314,     0.84359,     0.84348,       0.844,     0.84425,     0.84486,     0.84506,     0.84517,     0.84617,     0.84705,      0.8469,     0.84701,     0.84745,     0.84764,     0.84816,     0.84837,     0.84859,     0.84881,     0.84907,     0.84916,     0.84967,\n",
              "            0.85007,     0.85036,     0.85051,     0.85063,     0.85092,     0.85151,     0.85207,     0.85254,     0.85295,     0.85308,     0.85345,     0.85361,     0.85416,     0.85456,     0.85494,     0.85507,     0.85527,     0.85516,     0.85547,     0.85556,     0.85592,     0.85601,      0.8566,\n",
              "            0.85685,     0.85726,     0.85742,     0.85753,     0.85788,     0.85812,     0.85883,     0.85903,     0.85958,     0.85966,     0.85998,     0.86014,     0.86042,      0.8606,     0.86083,     0.86149,     0.86225,      0.8621,     0.86264,     0.86283,     0.86304,     0.86314,     0.86324,\n",
              "            0.86347,     0.86352,     0.86376,     0.86412,     0.86427,     0.86438,     0.86491,     0.86576,     0.86629,     0.86638,      0.8669,     0.86724,     0.86758,     0.86785,     0.86796,     0.86822,      0.8685,      0.8687,     0.86869,     0.86927,     0.86951,     0.86988,      0.8701,\n",
              "            0.87075,     0.87115,      0.8714,     0.87163,     0.87213,     0.87255,     0.87265,      0.8729,     0.87301,     0.87344,     0.87367,     0.87378,     0.87393,     0.87381,      0.8739,     0.87423,     0.87454,     0.87472,     0.87493,     0.87518,     0.87546,      0.8757,     0.87622,\n",
              "            0.87608,     0.87668,     0.87698,     0.87735,     0.87765,     0.87818,     0.87848,     0.87916,     0.87922,     0.87928,     0.87912,     0.87953,     0.87991,     0.88018,     0.88042,     0.88063,     0.88084,     0.88093,     0.88096,     0.88084,     0.88039,     0.88051,     0.88064,\n",
              "            0.88056,     0.88048,      0.8805,     0.88053,     0.88047,     0.88069,     0.88093,     0.88157,     0.88144,     0.88162,     0.88138,     0.88143,     0.88149,     0.88156,     0.88186,     0.88217,      0.8824,     0.88234,      0.8822,     0.88233,     0.88247,     0.88234,     0.88248,\n",
              "            0.88253,     0.88258,     0.88263,     0.88263,      0.8828,     0.88337,     0.88356,     0.88348,     0.88356,      0.8837,     0.88376,     0.88382,     0.88405,     0.88433,     0.88442,     0.88449,     0.88455,     0.88459,     0.88462,     0.88466,      0.8847,     0.88532,      0.8855,\n",
              "             0.8858,     0.88632,     0.88669,     0.88663,     0.88657,     0.88672,     0.88678,     0.88684,     0.88709,     0.88722,     0.88732,     0.88767,     0.88781,     0.88794,     0.88823,     0.88833,      0.8885,     0.88853,     0.88857,      0.8886,     0.88863,     0.88866,     0.88828,\n",
              "            0.88839,     0.88837,      0.8883,     0.88823,     0.88829,     0.88853,     0.88836,     0.88837,     0.88802,     0.88825,     0.88835,     0.88828,     0.88835,     0.88855,     0.88899,     0.88917,      0.8893,     0.88948,     0.89002,     0.88986,     0.88997,     0.88999,     0.89009,\n",
              "            0.89032,      0.8905,     0.89084,     0.89088,     0.89092,     0.89097,     0.89101,       0.891,     0.89108,     0.89116,     0.89125,     0.89153,     0.89149,     0.89166,     0.89184,     0.89208,     0.89225,     0.89211,     0.89216,     0.89202,     0.89228,     0.89267,     0.89268,\n",
              "            0.89263,     0.89259,     0.89254,     0.89261,     0.89324,     0.89317,     0.89327,     0.89356,     0.89366,     0.89387,     0.89408,     0.89423,      0.8944,     0.89452,     0.89498,     0.89531,      0.8953,     0.89558,     0.89592,     0.89589,     0.89598,      0.8961,     0.89653,\n",
              "            0.89687,     0.89675,     0.89695,     0.89708,     0.89731,     0.89705,     0.89664,     0.89703,     0.89736,     0.89752,     0.89764,     0.89776,     0.89759,     0.89777,     0.89825,     0.89833,     0.89842,     0.89912,      0.8993,     0.89939,      0.8997,     0.89982,     0.89994,\n",
              "            0.89985,     0.89976,     0.89975,     0.89999,     0.90021,     0.90015,         0.9,     0.89988,     0.89977,      0.8998,     0.89985,      0.8999,      0.9001,     0.90012,     0.90019,      0.9003,     0.90056,     0.90046,     0.90055,     0.90071,     0.90067,     0.90044,     0.90015,\n",
              "            0.89994,     0.89987,     0.89996,     0.90023,     0.90059,     0.90052,     0.90061,     0.90044,     0.90062,     0.90055,     0.90073,     0.90086,      0.9004,     0.90017,     0.90001,     0.89979,     0.90028,     0.90035,     0.90042,     0.90069,     0.90018,     0.90035,     0.90033,\n",
              "             0.9007,     0.90091,      0.9005,     0.90061,     0.90022,     0.90032,     0.90039,     0.90045,     0.90054,     0.90098,     0.90117,     0.90102,     0.90059,     0.90091,     0.90098,     0.90106,     0.90113,     0.90091,     0.90112,     0.90098,     0.90075,     0.90084,     0.90096,\n",
              "              0.901,     0.90035,     0.90045,     0.90073,     0.90085,     0.90093,     0.90094,     0.90063,     0.90025,     0.89995,      0.9001,     0.90009,     0.89994,     0.89979,     0.89983,     0.90024,     0.90054,      0.9009,     0.90075,      0.9004,     0.90025,     0.90061,     0.89982,\n",
              "            0.89991,     0.89988,      0.9001,     0.89949,      0.8997,     0.89974,     0.89965,     0.90001,     0.90013,     0.90017,     0.90026,     0.90011,     0.89989,     0.89983,     0.89957,     0.89962,     0.89969,     0.89952,     0.89937,     0.89946,     0.89939,     0.89931,     0.89923,\n",
              "            0.89924,     0.89935,     0.89951,     0.89926,     0.89936,     0.89884,     0.89911,     0.89874,     0.89911,     0.89814,     0.89816,     0.89801,     0.89835,     0.89771,     0.89741,     0.89713,     0.89725,     0.89729,     0.89744,     0.89718,     0.89714,     0.89702,      0.8964,\n",
              "            0.89646,     0.89637,     0.89637,     0.89657,     0.89624,     0.89641,      0.8964,     0.89628,     0.89635,     0.89674,     0.89652,     0.89619,     0.89605,     0.89619,     0.89632,     0.89632,     0.89585,     0.89558,     0.89544,     0.89547,     0.89452,     0.89428,     0.89399,\n",
              "            0.89318,     0.89308,     0.89232,     0.89231,     0.89233,     0.89102,     0.89025,     0.88929,      0.8881,     0.88752,     0.88748,     0.88704,     0.88695,     0.88599,     0.88545,     0.88467,     0.88432,     0.88445,     0.88338,     0.88259,     0.88205,     0.88131,     0.87964,\n",
              "            0.87944,     0.87891,     0.87766,     0.87693,     0.87658,     0.87455,     0.87331,     0.87167,     0.87132,     0.87022,      0.8694,     0.86844,     0.86752,     0.86668,       0.866,     0.86517,     0.86462,     0.86409,     0.86219,     0.86091,     0.86116,     0.85956,     0.85898,\n",
              "            0.85667,     0.85436,     0.85229,     0.85176,     0.84955,      0.8485,     0.84738,     0.84619,     0.84304,     0.84189,     0.84094,     0.83855,     0.83748,      0.8375,     0.83674,     0.83325,     0.83056,     0.82839,     0.82667,     0.82522,     0.82349,     0.82066,      0.8157,\n",
              "            0.81255,     0.80948,     0.80704,      0.8044,     0.80146,     0.79851,     0.79639,     0.79437,     0.79195,     0.78884,     0.78325,     0.78081,     0.77774,     0.77432,        0.77,     0.76675,     0.76347,     0.75986,      0.7571,     0.75238,     0.74894,     0.74316,     0.74004,\n",
              "            0.73704,     0.73177,     0.72895,      0.7236,     0.71901,     0.71603,     0.71204,     0.70554,     0.69948,      0.6947,     0.68954,     0.68504,     0.67824,     0.67171,     0.66261,     0.65755,      0.6499,     0.64327,     0.62927,     0.62144,     0.61151,     0.60614,      0.5959,\n",
              "            0.58929,     0.58018,     0.56905,     0.55673,     0.54445,     0.53168,     0.52352,     0.51081,     0.50041,     0.48639,     0.46956,     0.46004,     0.44568,      0.4314,     0.41888,     0.39752,     0.38772,     0.37525,     0.36503,     0.34987,     0.34233,     0.32943,     0.31689,\n",
              "            0.30823,     0.29478,      0.2842,     0.27399,     0.26436,      0.2539,     0.24332,     0.23585,     0.22384,     0.21353,     0.20831,     0.19386,     0.18186,     0.17308,     0.15599,     0.14626,     0.13714,     0.13217,     0.12433,     0.11282,     0.10482,    0.093846,    0.083447,\n",
              "           0.075939,    0.066859,    0.058457,    0.049981,    0.041447,    0.031249,    0.024905,    0.019315,    0.016884,    0.011251,   0.0092424,   0.0085719,    0.007192,   0.0055608,   0.0046158,   0.0022903,   0.0013016,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.014282,    0.014282,    0.014282,    0.014282,    0.014282,    0.014282,    0.014282,    0.028993,    0.041963,    0.049443,    0.056238,    0.062459,    0.068642,    0.074273,    0.080353,    0.085945,    0.091525,    0.097591,     0.10321,     0.10915,     0.11496,     0.12065,     0.12653,\n",
              "            0.13264,     0.13875,     0.14442,     0.15088,     0.15712,     0.16317,     0.16955,     0.17605,     0.18327,     0.18942,     0.19606,     0.20272,     0.20895,     0.21559,     0.22152,     0.22853,     0.23415,     0.24052,     0.24627,     0.25325,     0.25994,     0.26617,     0.27205,\n",
              "            0.27797,     0.28384,     0.28986,     0.29575,     0.30238,     0.30847,     0.31504,       0.321,     0.32718,     0.33234,     0.33843,     0.34286,     0.34862,     0.35269,     0.35798,     0.36374,     0.36867,     0.37335,     0.37853,     0.38269,     0.38766,     0.39303,     0.39867,\n",
              "            0.40289,     0.40767,     0.41289,     0.41713,      0.4216,     0.42568,     0.43048,      0.4341,     0.43931,      0.4448,     0.44866,     0.45324,     0.45759,     0.46235,      0.4661,     0.46983,     0.47465,     0.47859,     0.48222,     0.48659,     0.48992,     0.49328,     0.49803,\n",
              "            0.50171,     0.50599,     0.51012,     0.51442,     0.51811,     0.52206,     0.52514,     0.52791,     0.53083,     0.53497,     0.53809,     0.54331,     0.54636,     0.54871,     0.55184,      0.5537,     0.55685,     0.56057,     0.56512,     0.56705,     0.56994,     0.57301,     0.57508,\n",
              "            0.57732,     0.57987,       0.582,     0.58428,     0.58805,     0.59129,     0.59401,     0.59602,     0.59833,     0.60132,     0.60341,      0.6052,      0.6077,     0.61024,     0.61216,     0.61369,     0.61546,     0.61817,     0.62013,     0.62323,      0.6254,     0.62813,     0.63019,\n",
              "            0.63191,     0.63482,     0.63742,     0.64029,     0.64265,     0.64384,     0.64556,     0.64779,     0.64996,      0.6516,     0.65342,     0.65619,     0.65714,     0.65944,     0.66121,     0.66296,     0.66448,     0.66714,     0.67017,     0.67138,     0.67346,     0.67531,     0.67736,\n",
              "            0.67834,     0.68023,     0.68132,     0.68277,     0.68366,     0.68466,     0.68665,     0.68785,     0.68835,     0.69095,     0.69309,     0.69399,     0.69534,     0.69682,     0.69752,     0.69943,      0.7003,     0.70243,     0.70361,     0.70555,     0.70718,     0.70835,     0.71087,\n",
              "            0.71201,     0.71323,     0.71412,     0.71557,     0.71631,     0.71749,     0.71867,     0.71975,     0.72164,     0.72277,     0.72502,     0.72627,     0.72853,      0.7298,     0.73028,     0.73148,     0.73236,      0.7332,     0.73359,     0.73448,      0.7354,     0.73608,     0.73718,\n",
              "            0.73949,     0.73991,      0.7409,     0.74203,     0.74219,     0.74293,     0.74412,     0.74485,     0.74553,     0.74601,     0.74633,     0.74756,     0.74807,     0.74874,       0.749,      0.7495,     0.74983,     0.75017,      0.7516,     0.75247,     0.75355,     0.75399,     0.75459,\n",
              "             0.7551,     0.75566,     0.75625,     0.75698,     0.75733,     0.75816,     0.75883,     0.75981,     0.76025,     0.76056,     0.76219,     0.76361,     0.76363,     0.76381,     0.76453,     0.76484,     0.76568,     0.76602,     0.76638,     0.76674,     0.76716,     0.76731,     0.76815,\n",
              "            0.76881,     0.76928,     0.76952,     0.76999,     0.77053,     0.77169,     0.77262,      0.7734,     0.77407,     0.77455,     0.77515,     0.77542,     0.77634,     0.77699,     0.77762,     0.77812,      0.7785,     0.77853,     0.77904,     0.77919,     0.77979,     0.77994,     0.78091,\n",
              "            0.78134,     0.78201,     0.78229,     0.78258,     0.78332,     0.78373,     0.78491,     0.78525,     0.78617,     0.78658,     0.78711,     0.78739,     0.78785,     0.78844,     0.78939,     0.79049,     0.79179,     0.79181,     0.79272,     0.79304,      0.7934,     0.79356,     0.79373,\n",
              "            0.79413,     0.79448,     0.79491,     0.79551,     0.79576,     0.79595,     0.79714,     0.79858,     0.79948,     0.79964,     0.80053,     0.80111,     0.80169,     0.80244,     0.80293,     0.80337,     0.80384,     0.80418,     0.80447,     0.80545,     0.80587,     0.80651,     0.80688,\n",
              "            0.80801,     0.80899,     0.80942,     0.80981,     0.81067,     0.81141,     0.81189,     0.81231,     0.81251,     0.81325,     0.81365,     0.81384,     0.81409,     0.81415,     0.81435,     0.81492,     0.81546,     0.81578,     0.81615,     0.81658,     0.81707,     0.81748,     0.81843,\n",
              "            0.81845,     0.81949,     0.82003,     0.82067,     0.82149,     0.82244,     0.82297,     0.82415,     0.82426,     0.82436,     0.82435,     0.82512,     0.82578,     0.82658,     0.82699,     0.82737,     0.82774,     0.82807,     0.82833,     0.82829,     0.82821,     0.82873,     0.82898,\n",
              "            0.82895,     0.82893,     0.82918,     0.82939,     0.82961,     0.83024,     0.83074,     0.83188,     0.83229,      0.8326,      0.8327,     0.83321,     0.83333,     0.83346,       0.834,     0.83455,     0.83497,     0.83518,     0.83521,     0.83549,      0.8358,     0.83582,     0.83608,\n",
              "            0.83617,     0.83625,     0.83634,     0.83661,     0.83699,     0.83801,     0.83835,     0.83839,     0.83867,     0.83925,     0.83936,     0.83946,     0.83989,      0.8404,     0.84056,     0.84069,     0.84079,     0.84085,     0.84092,     0.84099,     0.84105,     0.84218,      0.8425,\n",
              "            0.84305,       0.844,     0.84466,     0.84474,     0.84478,     0.84505,     0.84515,     0.84526,     0.84572,     0.84622,     0.84648,     0.84711,     0.84737,     0.84795,     0.84847,     0.84865,     0.84897,     0.84903,     0.84908,     0.84914,      0.8492,     0.84926,     0.84924,\n",
              "            0.84943,     0.84947,     0.84945,     0.84943,      0.8496,     0.85005,     0.85005,     0.85041,     0.85078,     0.85121,     0.85174,     0.85195,     0.85218,     0.85278,      0.8536,     0.85394,     0.85417,     0.85451,     0.85584,     0.85586,     0.85617,     0.85649,     0.85667,\n",
              "            0.85709,     0.85743,     0.85806,     0.85814,     0.85821,     0.85829,     0.85837,     0.85871,     0.85886,     0.85901,     0.85917,     0.85969,     0.85997,     0.86064,     0.86096,     0.86141,     0.86178,     0.86174,     0.86192,     0.86201,      0.8625,     0.86322,      0.8633,\n",
              "            0.86329,     0.86328,     0.86327,     0.86347,     0.86465,     0.86485,     0.86506,      0.8656,     0.86579,     0.86619,     0.86658,     0.86686,     0.86718,      0.8674,     0.86826,      0.8689,      0.8691,     0.86976,     0.87041,     0.87071,     0.87088,      0.8711,     0.87191,\n",
              "            0.87298,     0.87306,     0.87343,     0.87369,     0.87412,      0.8742,      0.8743,     0.87504,     0.87568,     0.87597,      0.8762,     0.87643,     0.87685,      0.8772,     0.87812,     0.87826,     0.87843,     0.87978,     0.88011,     0.88029,     0.88089,     0.88148,     0.88175,\n",
              "            0.88174,     0.88172,     0.88202,     0.88257,     0.88299,     0.88333,      0.8833,     0.88327,     0.88325,     0.88333,     0.88342,     0.88351,     0.88394,     0.88432,     0.88446,     0.88466,     0.88527,     0.88535,     0.88553,     0.88583,     0.88614,     0.88617,     0.88611,\n",
              "            0.88624,     0.88637,     0.88666,     0.88719,      0.8879,     0.88813,     0.88831,     0.88836,     0.88872,     0.88934,      0.8897,     0.89002,     0.89025,      0.8902,     0.89022,     0.89057,     0.89152,     0.89167,     0.89182,     0.89272,     0.89276,     0.89376,     0.89399,\n",
              "            0.89472,     0.89513,     0.89508,     0.89572,       0.896,     0.89635,     0.89647,     0.89659,     0.89677,     0.89765,     0.89847,     0.89844,     0.89848,      0.8991,     0.89925,      0.8994,     0.89977,     0.89974,     0.90033,     0.90041,     0.90037,     0.90099,      0.9014,\n",
              "            0.90172,     0.90163,     0.90266,     0.90321,     0.90347,     0.90362,     0.90374,     0.90369,     0.90362,      0.9037,     0.90441,     0.90461,     0.90458,     0.90455,     0.90511,     0.90593,     0.90697,     0.90781,     0.90778,     0.90807,     0.90804,     0.90879,     0.90866,\n",
              "            0.90905,      0.9094,     0.91078,     0.91072,     0.91116,     0.91142,     0.91147,     0.91221,     0.91247,     0.91298,     0.91325,     0.91323,     0.91355,     0.91389,     0.91385,     0.91398,     0.91414,     0.91419,     0.91434,     0.91454,     0.91452,     0.91451,     0.91492,\n",
              "            0.91521,     0.91604,     0.91739,      0.9176,     0.91824,     0.91837,     0.91905,     0.91907,     0.92022,     0.92044,     0.92107,     0.92115,     0.92229,      0.9222,     0.92216,     0.92248,     0.92275,     0.92327,     0.92405,     0.92441,     0.92479,     0.92506,     0.92503,\n",
              "            0.92535,     0.92609,     0.92637,     0.92678,     0.92682,     0.92784,     0.92835,     0.92833,     0.92864,     0.92947,     0.92988,      0.9306,     0.93173,     0.93252,      0.9328,      0.9333,     0.93324,     0.93321,     0.93374,     0.93397,     0.93385,      0.9341,     0.93417,\n",
              "            0.93506,     0.93562,     0.93553,     0.93592,     0.93661,     0.93656,     0.93686,     0.93675,      0.9366,     0.93733,     0.93853,      0.9393,     0.93968,     0.93957,     0.93991,     0.94023,     0.94101,     0.94184,     0.94254,     0.94287,     0.94323,     0.94315,     0.94323,\n",
              "            0.94419,     0.94414,     0.94442,     0.94554,     0.94685,     0.94707,     0.94694,     0.94809,     0.94894,     0.94923,     0.94959,     0.95004,     0.95028,      0.9502,     0.95014,      0.9505,     0.95066,     0.95128,     0.95155,     0.95143,     0.95326,     0.95356,     0.95397,\n",
              "            0.95376,     0.95355,     0.95336,     0.95331,     0.95404,     0.95395,     0.95431,     0.95514,     0.95487,     0.95714,     0.95802,     0.95783,     0.95774,     0.95919,      0.9606,     0.96033,     0.96011,     0.95994,     0.96031,     0.96069,     0.96106,     0.96135,     0.96097,\n",
              "            0.96124,     0.96257,     0.96344,     0.96378,      0.9641,     0.96442,     0.96482,     0.96467,     0.96505,     0.96593,     0.96611,     0.96594,     0.96629,     0.96836,     0.96866,     0.96963,     0.97061,     0.97159,     0.97258,     0.97299,      0.9728,      0.9731,     0.97419,\n",
              "            0.97466,     0.97503,      0.9775,     0.97791,      0.9777,     0.97755,     0.97736,     0.97774,     0.97885,     0.97864,      0.9784,     0.97818,      0.9786,     0.97904,     0.97938,     0.97915,     0.97879,     0.97848,     0.97865,     0.97907,     0.97871,      0.9792,      0.9787,\n",
              "            0.97928,     0.97976,     0.98114,     0.98157,       0.981,     0.98145,     0.98215,     0.98269,     0.98301,     0.98274,     0.98194,     0.98146,     0.98205,      0.9813,      0.9806,     0.98086,     0.98188,     0.98451,     0.98398,     0.98498,     0.98839,     0.98785,     0.98728,\n",
              "            0.98686,     0.98616,     0.98794,     0.98742,     0.98948,     0.98899,     0.98845,       0.991,     0.99361,     0.99327,     0.99308,     0.99251,     0.99197,     0.99152,     0.99051,     0.98984,     0.98912,     0.98868,     0.98793,     0.98663,     0.98557,     0.99184,     0.99078,\n",
              "            0.98984,     0.98842,     0.98672,     0.98444,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
              "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
              "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
              "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
              "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
              "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
              "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
              "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
              "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
              "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
              "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
              "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
              "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
              "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
              "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
              "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
              "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
              "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
              "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
              "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
              "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
              "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
              "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
              "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
              "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
              "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
              "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
              "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
              "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
              "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
              "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
              "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
              "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
              "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
              "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
              "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
              "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
              "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
              "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
              "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
              "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
              "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
              "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.98824,     0.98824,     0.98824,     0.98824,     0.98824,     0.98824,     0.98824,     0.98703,      0.9846,     0.98216,     0.98135,     0.98095,     0.97973,     0.97973,     0.97973,     0.97933,     0.97933,     0.97933,     0.97933,     0.97933,     0.97892,     0.97852,     0.97811,\n",
              "            0.97811,     0.97771,     0.97649,     0.97608,     0.97568,     0.97568,     0.97568,     0.97568,     0.97568,     0.97568,     0.97527,     0.97446,     0.97365,     0.97365,     0.97365,     0.97365,     0.97365,     0.97365,     0.97365,     0.97365,     0.97325,     0.97325,     0.97325,\n",
              "            0.97325,     0.97284,     0.97284,     0.97284,     0.97284,     0.97284,     0.97244,     0.97244,     0.97244,     0.97163,     0.97163,     0.97163,     0.97163,     0.97122,     0.97081,     0.97081,     0.97081,      0.9696,      0.9696,      0.9696,      0.9696,      0.9696,      0.9696,\n",
              "            0.96919,     0.96919,     0.96919,     0.96919,     0.96879,     0.96838,     0.96838,     0.96838,     0.96838,     0.96838,     0.96838,     0.96838,     0.96838,     0.96838,     0.96798,     0.96798,     0.96757,     0.96757,     0.96757,     0.96757,     0.96717,     0.96676,     0.96636,\n",
              "            0.96595,     0.96595,     0.96555,     0.96555,     0.96555,     0.96555,     0.96555,     0.96555,     0.96555,     0.96473,     0.96473,     0.96473,     0.96473,     0.96433,     0.96392,     0.96392,     0.96392,     0.96392,     0.96392,     0.96392,     0.96392,     0.96352,     0.96352,\n",
              "            0.96352,     0.96352,     0.96352,     0.96352,     0.96311,     0.96271,     0.96271,      0.9619,     0.96149,     0.96149,     0.96149,     0.96109,     0.96109,     0.96109,     0.96109,     0.96109,     0.96068,     0.96028,     0.96028,     0.96028,     0.96028,     0.95987,     0.95987,\n",
              "            0.95987,     0.95987,     0.95987,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95946,     0.95865,     0.95865,\n",
              "            0.95865,     0.95865,     0.95865,     0.95865,     0.95865,     0.95865,     0.95825,     0.95784,     0.95744,     0.95744,     0.95744,     0.95744,     0.95703,     0.95622,     0.95622,     0.95622,     0.95622,     0.95622,     0.95622,     0.95622,     0.95622,     0.95622,     0.95622,\n",
              "            0.95622,     0.95622,     0.95622,     0.95582,     0.95541,     0.95541,     0.95541,     0.95541,     0.95501,     0.95501,     0.95501,     0.95501,     0.95501,     0.95501,     0.95501,     0.95501,       0.955,      0.9546,      0.9546,      0.9546,      0.9546,      0.9546,      0.9546,\n",
              "             0.9546,      0.9546,      0.9546,      0.9546,      0.9546,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,      0.9542,     0.95379,     0.95379,     0.95379,     0.95379,     0.95379,     0.95379,     0.95301,     0.95298,     0.95298,     0.95298,     0.95298,     0.95298,\n",
              "            0.95298,     0.95273,     0.95257,     0.95257,     0.95176,     0.95176,     0.95136,     0.95136,     0.95118,     0.95095,     0.95095,     0.95095,     0.95055,     0.95055,     0.95055,     0.95055,     0.95055,     0.95055,     0.95055,     0.95055,     0.95055,     0.95055,     0.95055,\n",
              "            0.95055,     0.95055,     0.95055,     0.95014,     0.95004,     0.94974,     0.94974,     0.94974,     0.94974,     0.94933,     0.94933,     0.94933,     0.94933,     0.94933,     0.94933,     0.94893,     0.94883,     0.94852,     0.94852,     0.94852,     0.94852,     0.94852,     0.94852,\n",
              "            0.94852,     0.94852,     0.94852,     0.94836,     0.94812,     0.94812,     0.94812,     0.94812,     0.94812,     0.94771,     0.94771,     0.94771,     0.94771,      0.9473,     0.94649,     0.94649,     0.94647,     0.94609,     0.94609,     0.94609,     0.94609,     0.94609,     0.94609,\n",
              "            0.94609,     0.94568,     0.94568,     0.94568,     0.94568,     0.94568,     0.94528,     0.94528,     0.94528,     0.94528,     0.94528,     0.94528,     0.94528,     0.94487,     0.94447,     0.94447,     0.94447,     0.94447,     0.94406,     0.94406,     0.94406,     0.94406,     0.94406,\n",
              "            0.94406,     0.94366,     0.94366,     0.94366,     0.94366,     0.94366,     0.94325,     0.94325,     0.94325,     0.94325,     0.94325,     0.94325,     0.94325,     0.94291,     0.94285,     0.94285,     0.94285,     0.94285,     0.94285,     0.94285,     0.94285,     0.94285,      0.9428,\n",
              "            0.94244,     0.94244,     0.94244,     0.94244,     0.94204,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94169,     0.94163,     0.94163,     0.94122,     0.94122,     0.94122,     0.94122,     0.94101,     0.94074,      0.9405,      0.9396,      0.9392,     0.93917,\n",
              "            0.93902,     0.93887,     0.93858,     0.93839,     0.93798,     0.93767,     0.93758,     0.93758,     0.93677,     0.93677,     0.93611,     0.93556,     0.93555,     0.93555,     0.93555,     0.93555,     0.93555,     0.93514,     0.93479,     0.93474,     0.93465,     0.93433,     0.93433,\n",
              "            0.93433,     0.93433,     0.93433,       0.934,     0.93393,     0.93393,     0.93393,      0.9337,     0.93352,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,     0.93312,\n",
              "            0.93312,     0.93312,     0.93312,      0.9329,     0.93271,     0.93271,     0.93271,     0.93271,     0.93271,      0.9324,     0.93231,     0.93231,     0.93231,      0.9319,      0.9319,      0.9319,      0.9319,      0.9319,      0.9319,      0.9319,      0.9319,      0.9319,     0.93109,\n",
              "            0.93109,       0.931,     0.93087,     0.93075,     0.93069,     0.93066,     0.93028,     0.92987,     0.92866,     0.92866,     0.92825,     0.92785,     0.92772,     0.92744,     0.92744,     0.92744,     0.92744,     0.92744,     0.92704,     0.92667,     0.92656,     0.92623,     0.92623,\n",
              "            0.92623,     0.92623,     0.92623,     0.92623,     0.92623,     0.92623,     0.92623,     0.92582,     0.92582,     0.92582,     0.92582,     0.92582,     0.92542,     0.92501,     0.92501,     0.92501,     0.92497,     0.92469,      0.9246,     0.92421,      0.9242,      0.9242,     0.92413,\n",
              "            0.92404,     0.92396,     0.92387,     0.92379,     0.92379,     0.92342,     0.92339,     0.92339,     0.92339,     0.92339,     0.92339,     0.92339,     0.92339,     0.92339,     0.92339,     0.92339,     0.92313,     0.92298,     0.92297,     0.92258,     0.92258,     0.92258,     0.92258,\n",
              "            0.92211,     0.92177,     0.92177,     0.92177,     0.92177,     0.92113,     0.92015,     0.92015,     0.92015,     0.92015,     0.92015,     0.92015,     0.91934,     0.91934,     0.91934,     0.91934,     0.91934,     0.91934,     0.91934,     0.91934,     0.91934,     0.91893,      0.9189,\n",
              "            0.91873,     0.91856,      0.9182,     0.91812,     0.91812,     0.91762,     0.91734,     0.91712,     0.91692,      0.9169,      0.9169,      0.9169,     0.91687,      0.9165,      0.9165,      0.9165,     0.91639,     0.91609,     0.91609,     0.91609,     0.91569,     0.91517,     0.91464,\n",
              "            0.91407,     0.91379,     0.91366,     0.91366,     0.91366,     0.91325,     0.91325,     0.91286,     0.91285,     0.91204,     0.91204,     0.91197,     0.91078,     0.91036,     0.91001,      0.9092,      0.9092,      0.9092,      0.9092,      0.9088,     0.90773,     0.90704,     0.90677,\n",
              "            0.90677,     0.90677,     0.90598,     0.90555,     0.90448,     0.90434,     0.90434,     0.90434,     0.90434,     0.90434,     0.90389,     0.90362,     0.90272,     0.90272,     0.90272,     0.90272,     0.90248,     0.90209,     0.90191,     0.90155,     0.90114,     0.90069,     0.90051,\n",
              "            0.90028,     0.89908,     0.89826,     0.89826,     0.89826,     0.89826,     0.89816,     0.89759,     0.89691,     0.89623,     0.89582,     0.89562,     0.89535,     0.89507,     0.89461,     0.89461,      0.8942,      0.8941,     0.89382,     0.89287,     0.89259,     0.89258,     0.89116,\n",
              "            0.89096,     0.89056,     0.88967,     0.88853,     0.88853,     0.88835,     0.88812,     0.88812,     0.88812,     0.88772,     0.88764,     0.88736,     0.88663,     0.88621,     0.88574,     0.88569,     0.88569,     0.88531,     0.88488,     0.88488,     0.88474,      0.8846,     0.88407,\n",
              "            0.88382,     0.88326,     0.88231,     0.88164,     0.88123,     0.88012,     0.88002,     0.87929,     0.87895,      0.8769,     0.87637,     0.87601,     0.87563,     0.87449,     0.87396,     0.87313,     0.87313,     0.87272,     0.87231,      0.8715,      0.8711,     0.87064,     0.86948,\n",
              "            0.86932,     0.86849,     0.86826,     0.86826,     0.86762,     0.86704,     0.86657,     0.86636,     0.86623,     0.86623,     0.86547,     0.86423,       0.863,     0.86259,     0.86259,     0.86215,     0.86133,     0.86087,     0.86015,     0.86002,     0.85838,     0.85772,     0.85712,\n",
              "            0.85488,     0.85424,     0.85293,     0.85258,     0.85205,      0.8497,     0.84806,     0.84642,     0.84437,     0.84273,      0.8417,     0.84029,     0.83983,     0.83819,     0.83695,     0.83531,     0.83408,     0.83365,      0.8312,     0.82956,     0.82832,     0.82708,     0.82408,\n",
              "            0.82299,      0.8221,     0.81971,     0.81759,     0.81602,     0.81235,      0.8103,     0.80665,     0.80543,     0.80335,     0.80171,     0.79976,     0.79802,     0.79665,     0.79555,      0.7939,     0.79287,     0.79154,     0.78817,     0.78612,     0.78529,     0.78243,      0.7812,\n",
              "            0.77753,     0.77386,      0.7706,     0.76977,     0.76569,     0.76405,       0.762,     0.75955,     0.75467,      0.7514,     0.74936,     0.74569,     0.74405,     0.74322,     0.74117,     0.73588,      0.7318,     0.72854,     0.72568,     0.72323,     0.72037,     0.71589,     0.70858,\n",
              "            0.70369,      0.6984,     0.69433,     0.69026,     0.68578,      0.6813,     0.67803,     0.67518,     0.67151,     0.66662,      0.6586,     0.65523,     0.65075,     0.64506,     0.63896,     0.63407,     0.62919,      0.6239,     0.61978,     0.61332,     0.60884,     0.60112,     0.59664,\n",
              "            0.59257,     0.58565,     0.58118,     0.57426,     0.56857,      0.5649,     0.56002,     0.55189,     0.54417,     0.53847,     0.53237,     0.52708,     0.51895,     0.51123,     0.50067,     0.49498,     0.48645,     0.47913,     0.46372,     0.45517,     0.44467,     0.43892,     0.42836,\n",
              "            0.42145,      0.4121,     0.40073,     0.38855,     0.37678,      0.3646,     0.35687,      0.3451,     0.33563,     0.32317,     0.30856,     0.30043,     0.28825,     0.27647,     0.26632,     0.24927,     0.24155,      0.2318,     0.22408,     0.21271,     0.20702,     0.19767,     0.18873,\n",
              "            0.18263,     0.17329,     0.16597,     0.15906,     0.15256,     0.14565,     0.13873,     0.13385,     0.12613,     0.11962,     0.11636,     0.10742,      0.1001,    0.094815,    0.084661,    0.078965,    0.073676,    0.070818,    0.066339,    0.059833,    0.055354,    0.049253,    0.043558,\n",
              "           0.039484,      0.0346,    0.030121,    0.025641,    0.021162,    0.015872,    0.012609,   0.0097515,   0.0085138,   0.0056575,   0.0046427,   0.0043044,    0.003609,   0.0027882,   0.0023132,   0.0011465,  0.00065124,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
              "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
              "fitness: np.float64(0.5558446713018518)\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([    0.51379])\n",
              "names: {0: 'person'}\n",
              "nt_per_class: array([2467])\n",
              "nt_per_image: array([352])\n",
              "results_dict: {'metrics/precision(B)': np.float64(0.8965918446062753), 'metrics/recall(B)': np.float64(0.9043372517227402), 'metrics/mAP50(B)': np.float64(0.9342975361473422), 'metrics/mAP50-95(B)': np.float64(0.5137943529856861), 'fitness': np.float64(0.5558446713018518)}\n",
              "save_dir: PosixPath('/content/drive/MyDrive/runs/detect/train32')\n",
              "speed: {'preprocess': 0.28730945617242376, 'inference': 16.498858343868307, 'loss': 0.0007124578890701134, 'postprocess': 0.6267710806959689}\n",
              "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
              "task: 'detect'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "rtdetr_checkpoint = os.path.join(PATH, 'train3/weights/last.pt')\n",
        "\n",
        "if os.path.exists(rtdetr_checkpoint):\n",
        "    print(\"âœ… Resuming RT-DETR-L training...\")\n",
        "    model_path = rtdetr_checkpoint\n",
        "    resume_training = True\n",
        "else:\n",
        "    print(\"ğŸš€ Starting new RT-DETR-L training...\")\n",
        "    model_path = 'rtdetr-l.pt'\n",
        "    resume_training = False\n",
        "\n",
        "modelRTDETR = RTDETR(model_path)\n",
        "\n",
        "modelRTDETR.train(\n",
        "    project=PATH,\n",
        "    name=\"train3\",\n",
        "    resume=resume_training,\n",
        "    data= locDataset,\n",
        "    epochs=50,\n",
        "    batch=8,\n",
        "    lr0=0.0001,  # Sets the initial learning rate\n",
        "    imgsz=640\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7aTH2LkNaud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e95563-b258-4d53-af7b-58dfb29dcd05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n",
            "Evaluating YOLOv12...\n",
            "Ultralytics 8.3.167 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLOv12l summary (fused): 283 layers, 26,339,843 parameters, 0 gradients, 88.5 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 12.8Â±5.6 MB/s, size: 33.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Human-Thermal-Detection-5/test/labels... 288 images, 114 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [00:00<00:00, 975.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Human-Thermal-Detection-5/test/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:17<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        288       1312      0.897      0.868      0.931      0.519\n",
            "Speed: 1.7ms preprocess, 52.2ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Saving /content/drive/MyDrive/runs/detect/train42/predictions.json...\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/runs/detect/train42\u001b[0m\n",
            "Evaluating RT-DETR...\n",
            "Ultralytics 8.3.167 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "rt-detr-l summary: 302 layers, 31,985,795 parameters, 0 gradients, 103.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1208.0Â±494.9 MB/s, size: 30.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Human-Thermal-Detection-5/test/labels.cache... 288 images, 114 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:12<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        288       1312      0.913        0.9      0.948      0.524\n",
            "Speed: 0.2ms preprocess, 35.1ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
            "Saving /content/drive/MyDrive/runs/detect/train32/predictions.json...\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/runs/detect/train32\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from sys import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading models...\")\n",
        "model_yolo = YOLO(os.path.join(PATH, 'train4/weights/best.pt'))\n",
        "model_rtdetr = RTDETR(os.path.join(PATH, 'train3/weights/best.pt'))\n",
        "\n",
        "# Validate models on the test set to get mAP and Latency\n",
        "print(\"Evaluating YOLOv12...\")\n",
        "yolo_metrics = model_yolo.val(\n",
        "    project=PATH,\n",
        "    name=\"train4\",\n",
        "    split='test',\n",
        "    data=locDataset,\n",
        "    save_json=True\n",
        ")\n",
        "\n",
        "print(\"Evaluating RT-DETR...\")\n",
        "rtdetr_metrics = model_rtdetr.val(\n",
        "    project=PATH,\n",
        "    name=\"train3\",\n",
        "    split='test',\n",
        "    data=locDataset,\n",
        "    save_json=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Get GFLOPs\n",
        "yolo_flops = model_yolo.info()[3]\n",
        "rtdetr_flops = model_rtdetr.info()[3]\n",
        "\n",
        "# Store the stats\n",
        "stats = {\n",
        "    'YOLOv12': {\n",
        "        'mAP': yolo_metrics.box.map * 100,  # Convert to percentage\n",
        "        'Latency': yolo_metrics.speed['inference'],\n",
        "        'FLOPs': yolo_flops\n",
        "    },\n",
        "    'RT-DETRv2': {\n",
        "        'mAP': rtdetr_metrics.box.map * 100,\n",
        "        'Latency': rtdetr_metrics.speed['inference'],\n",
        "        'FLOPs': rtdetr_flops\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n--- Collected Statistics ---\")\n",
        "print(stats)\n",
        "\n",
        "print(\"\\n--- Model Comparison Table ---\")\n",
        "# Create a DataFrame from the dictionary\n",
        "df_stats = pd.DataFrame.from_dict(stats, orient='index')\n",
        "\n",
        "# Rename columns for clarity in the table\n",
        "df_stats = df_stats.rename(columns={\n",
        "    'mAP': 'mAP50-95 (%)',\n",
        "    'Latency': 'Latency (ms)',\n",
        "    'FLOPs': 'FLOPs (G)'\n",
        "})\n",
        "display(df_stats.round(2))\n",
        "\n",
        "print(\"\\n--- Model Comparison Graph ---\")\n",
        "# Plot the Comparison Graphs\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "models = list(stats.keys())\n",
        "colors = ['#e41a1c', '#377eb8'] # Red for YOLO, Blue for RT-DETR\n",
        "\n",
        "# Plot 1: mAP50-95 vs. Latency\n",
        "ax[0].set_title('mAP50-95 vs. Latency', fontsize=16)\n",
        "ax[0].set_xlabel('Latency (ms)', fontsize=12)\n",
        "ax[0].set_ylabel('mAP50-95  (%)', fontsize=12)\n",
        "ax[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    ax[0].scatter(stats[model]['Latency'], stats[model]['mAP'], s=150, color=colors[i], label=model, zorder=3)\n",
        "    ax[0].text(stats[model]['Latency'] + 0.2, stats[model]['mAP'], model, fontsize=12)\n",
        "\n",
        "# Plot 2: mAP50-95 vs. FLOPs\n",
        "ax[1].set_title('mAP50-95  vs. FLOPs', fontsize=16)\n",
        "ax[1].set_xlabel('FLOPs (G)', fontsize=12)\n",
        "ax[1].set_ylabel('mAP50-95  (%)', fontsize=12)\n",
        "ax[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    ax[1].scatter(stats[model]['FLOPs'], stats[model]['mAP'], s=150, color=colors[i], label=model, zorder=3)\n",
        "    ax[1].text(stats[model]['FLOPs'] + 2, stats[model]['mAP'], model, fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "7ixt5SvuUz9m",
        "outputId": "1e4ef569-85c1-4876-d81e-36aeab62f231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv12l summary (fused): 283 layers, 26,339,843 parameters, 0 gradients, 88.5 GFLOPs\n",
            "rt-detr-l summary: 302 layers, 31,985,795 parameters, 0 gradients, 103.4 GFLOPs\n",
            "\n",
            "--- Collected Statistics ---\n",
            "{'YOLOv12': {'mAP': np.float64(51.87665777904614), 'Latency': 52.23965534027128, 'FLOPs': 88.54594559999998}, 'RT-DETRv2': {'mAP': np.float64(52.3691446119686), 'Latency': 35.06740894100505, 'FLOPs': 103.43296}}\n",
            "\n",
            "--- Model Comparison Table ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           mAP50-95 (%)  Latency (ms)  FLOPs (G)\n",
              "YOLOv12           51.88         52.24      88.55\n",
              "RT-DETRv2         52.37         35.07     103.43"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13b565c2-747b-449a-b162-d373ecf94eb2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mAP50-95 (%)</th>\n",
              "      <th>Latency (ms)</th>\n",
              "      <th>FLOPs (G)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>YOLOv12</th>\n",
              "      <td>51.88</td>\n",
              "      <td>52.24</td>\n",
              "      <td>88.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RT-DETRv2</th>\n",
              "      <td>52.37</td>\n",
              "      <td>35.07</td>\n",
              "      <td>103.43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13b565c2-747b-449a-b162-d373ecf94eb2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13b565c2-747b-449a-b162-d373ecf94eb2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13b565c2-747b-449a-b162-d373ecf94eb2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-413af4e1-b1f7-435e-acde-a1efdd0f29d1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-413af4e1-b1f7-435e-acde-a1efdd0f29d1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-413af4e1-b1f7-435e-acde-a1efdd0f29d1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"plt\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"mAP50-95 (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3464823227814047,\n        \"min\": 51.88,\n        \"max\": 52.37,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          52.37,\n          51.88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latency (ms)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.141023432973022,\n        \"min\": 35.07,\n        \"max\": 52.24,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          35.07,\n          52.24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FLOPs (G)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.521748904055833,\n        \"min\": 88.55,\n        \"max\": 103.43,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          103.43,\n          88.55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Comparison Graph ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}